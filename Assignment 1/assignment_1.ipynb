{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "6OBJyXJaTZzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing LaTeX. This is not mandatory, and it is used to plot using LaTex.\n",
        "!sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended cm-super"
      ],
      "metadata": {
        "id": "S2JMHpBiZBg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6235960c-e658-49fb-c209-41da938b4d5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  cm-super-minimal fonts-droid-fallback fonts-lato fonts-lmodern\n",
            "  fonts-noto-mono fonts-texgyre ghostscript gsfonts javascript-common\n",
            "  libcupsfilters1 libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0\n",
            "  libjs-jquery libkpathsea6 libpotrace0 libptexenc1 libruby2.5 libsynctex1\n",
            "  libtexlua52 libtexluajit2 libzzip-0-13 lmodern pfb2t1c2pfb poppler-data\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-latex-base texlive-latex-recommended\n",
            "  texlive-pictures texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  fonts-noto ghostscript-x apache2 | lighttpd | httpd poppler-utils\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic\n",
            "  | fonts-ipafont-gothic fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri\n",
            "  ruby-dev bundler debhelper perl-tk xpdf-reader | pdf-viewer\n",
            "  texlive-fonts-recommended-doc texlive-latex-base-doc python-pygments\n",
            "  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl\n",
            "  texlive-latex-extra-doc texlive-latex-recommended-doc texlive-pstricks\n",
            "  dot2tex prerex ruby-tcltk | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  cm-super cm-super-minimal dvipng fonts-droid-fallback fonts-lato\n",
            "  fonts-lmodern fonts-noto-mono fonts-texgyre ghostscript gsfonts\n",
            "  javascript-common libcupsfilters1 libcupsimage2 libgs9 libgs9-common\n",
            "  libijs-0.35 libjbig2dec0 libjs-jquery libkpathsea6 libpotrace0 libptexenc1\n",
            "  libruby2.5 libsynctex1 libtexlua52 libtexluajit2 libzzip-0-13 lmodern\n",
            "  pfb2t1c2pfb poppler-data preview-latex-style rake ruby ruby-did-you-mean\n",
            "  ruby-minitest ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-fonts-recommended texlive-latex-base\n",
            "  texlive-latex-extra texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa\n",
            "0 upgraded, 51 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 163 MB of archives.\n",
            "After this operation, 503 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.9 [18.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.17 [5,092 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.17 [2,267 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super-minimal all 0.3.4-11 [5,810 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pfb2t1c2pfb amd64 0.3-11 [9,342 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super all 0.3.4-11 [18.7 MB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.17 [51.3 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dvipng amd64 1.15-1 [78.2 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.12 [48.6 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.12 [3,073 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Fetched 163 MB in 5s (30.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 51.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 123991 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../04-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../05-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../06-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../07-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../08-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../09-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../10-libcupsimage2_2.2.7-1ubuntu2.9_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.9) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../11-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../12-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../13-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.17_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../14-libgs9_9.26~dfsg+0-0ubuntu0.18.04.17_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../15-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../16-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../17-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../18-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../19-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../20-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../21-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package cm-super-minimal.\n",
            "Preparing to unpack .../22-cm-super-minimal_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super-minimal (0.3.4-11) ...\n",
            "Selecting previously unselected package pfb2t1c2pfb.\n",
            "Preparing to unpack .../23-pfb2t1c2pfb_0.3-11_amd64.deb ...\n",
            "Unpacking pfb2t1c2pfb (0.3-11) ...\n",
            "Selecting previously unselected package cm-super.\n",
            "Preparing to unpack .../24-cm-super_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super (0.3.4-11) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../25-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.17_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Selecting previously unselected package dvipng.\n",
            "Preparing to unpack .../26-dvipng_1.15-1_amd64.deb ...\n",
            "Unpacking dvipng (1.15-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../27-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../28-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../29-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../30-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../31-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../32-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../33-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../34-ruby2.5_2.5.1-1ubuntu1.12_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.12) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../35-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../36-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../37-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../38-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../39-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../40-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../41-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../42-libruby2.5_2.5.1-1ubuntu1.12_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.12) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../43-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../44-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../45-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../46-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../47-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../48-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../49-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../50-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up pfb2t1c2pfb (0.3-11) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.9) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.17) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up dvipng (1.15-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up cm-super-minimal (0.3.4-11) ...\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up cm-super (0.3.4-11) ...\n",
            "Creating fonts. This may take some time... done.\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.12) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.12) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C7sW4HdzTURu"
      },
      "outputs": [],
      "source": [
        "# Importing os.\n",
        "import os\n",
        "\n",
        "# Importing urllib.request.\n",
        "import urllib.request\n",
        "\n",
        "# Importing zipfile.\n",
        "import zipfile\n",
        "\n",
        "# Importing pandas.\n",
        "import pandas as pd\n",
        "\n",
        "# Importing numpy.\n",
        "import numpy as np\n",
        "\n",
        "# Importing random.\n",
        "import random\n",
        "\n",
        "# Importing tensorflow.\n",
        "import tensorflow as tf\n",
        "\n",
        "# Importing pad_sequences.\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Importing Sequential.\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Importing Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation.\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation, GRU\n",
        "\n",
        "# Importing L2.\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Importing Adam.\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Importing EarlyStopping and ReduceLROnPlateau.\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Importing classification_report.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Importing pyplot.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sets reproducibility.\n",
        "def set_reproducibility(seed):\n",
        "\n",
        "  # Setting seeds.\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "  os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "  os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "\n",
        "# Setting seed.\n",
        "set_reproducibility(seed = 42)\n",
        "\n",
        "# Using TeX. This is not mandatory, and it is used to plot using LaTex.\n",
        "plt.rc(\"text\", usetex = True)\n",
        "\n",
        "# Setting the font family. This is not mandatory, and it is used to plot using LaTex.\n",
        "plt.rc(\"font\", family = \"serif\")\n",
        "\n",
        "# Setting the font size. This is not mandatory, and it is used to plot using LaTex.\n",
        "plt.rcParams.update({\"font.size\": 15})\n",
        "\n",
        "# Using package amsmath. This is not mandatory, and it is used to plot using LaTex.\n",
        "plt.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "c9thEzbNTfiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to download .zips.\n",
        "def downloader(url, folder_name, filename):\n",
        "\n",
        "  # Defining data folder path.\n",
        "  data_path = os.path.join(os.getcwd(), folder_name)\n",
        "\n",
        "  # Creating data folder.\n",
        "  if not os.path.exists(data_path):\n",
        "      os.makedirs(data_path)\n",
        "\n",
        "  # Defining .zip file path.\n",
        "  zip_path = os.path.join(os.getcwd(), folder_name, filename)\n",
        "\n",
        "  # Requesting .zip file.\n",
        "  if not os.path.exists(zip_path):\n",
        "      urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "  # Extracting data from .zip.\n",
        "  with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "      zip_ref.extractall(path = data_path)\n",
        "\n",
        "  # Returning data_path and zip_path.\n",
        "  return data_path, zip_path"
      ],
      "metadata": {
        "id": "hOWsBikzhqq8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading dataset.\n",
        "data_path, _ = downloader(url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\", folder_name = \"data\", filename = \"dependency_treebank.zip\")\n",
        "\n",
        "# Downloading glove.\n",
        "glove_path, _ = downloader(url = \"https://nlp.stanford.edu/data/glove.6B.zip\", folder_name = \"glove\", filename = \"glove.6B.zip\")"
      ],
      "metadata": {
        "id": "7f3f0k3KTcr3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the dataset name.\n",
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "# Defining path to first training sample.\n",
        "file_path = os.path.join(data_path, dataset_name, \"wsj_0001.dp\")\n",
        "\n",
        "# Reading first training sample.\n",
        "if os.path.isfile(file_path):\n",
        "\n",
        "  # Printing file.\n",
        "  with open(file_path, mode = \"r\") as text_file: print(text_file.read())"
      ],
      "metadata": {
        "id": "QZDEgowtTqYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29335d32-2e5f-4b1b-9ae8-e6a8b9028e02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pierre\tNNP\t2\n",
            "Vinken\tNNP\t8\n",
            ",\t,\t2\n",
            "61\tCD\t5\n",
            "years\tNNS\t6\n",
            "old\tJJ\t2\n",
            ",\t,\t2\n",
            "will\tMD\t0\n",
            "join\tVB\t8\n",
            "the\tDT\t11\n",
            "board\tNN\t9\n",
            "as\tIN\t9\n",
            "a\tDT\t15\n",
            "nonexecutive\tJJ\t15\n",
            "director\tNN\t12\n",
            "Nov.\tNNP\t9\n",
            "29\tCD\t16\n",
            ".\t.\t8\n",
            "\n",
            "Mr.\tNNP\t2\n",
            "Vinken\tNNP\t3\n",
            "is\tVBZ\t0\n",
            "chairman\tNN\t3\n",
            "of\tIN\t4\n",
            "Elsevier\tNNP\t7\n",
            "N.V.\tNNP\t12\n",
            ",\t,\t12\n",
            "the\tDT\t12\n",
            "Dutch\tNNP\t12\n",
            "publishing\tVBG\t12\n",
            "group\tNN\t5\n",
            ".\t.\t3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining embedding size.\n",
        "EMBEDDING_SIZE = 50\n",
        "\n",
        "# Defining specific glove's file path.\n",
        "glove_file = os.path.join(os.getcwd(), glove_path, f\"glove.6B.{str(EMBEDDING_SIZE)}d.txt\")\n",
        "\n",
        "# Reading lines of file.\n",
        "with open(glove_file, encoding = \"utf8\" ) as text_file: \n",
        "  lines = text_file.readlines()\n",
        "\n",
        "# Defining initial vocabulary.\n",
        "embedding_vocabulary = {}\n",
        "\n",
        "# Reading single lines.\n",
        "for line in lines:\n",
        "\n",
        "  # Splitting line.\n",
        "  splits = line.split()\n",
        "\n",
        "  # Storing line into vocabulary.\n",
        "  embedding_vocabulary[splits[0]] = np.array([float(val) for val in splits[1:]])\n",
        "\n",
        "# Printing one entry of the vocabulary.\n",
        "print(\"The embedding for 'the' is:\\n{}.\".format(embedding_vocabulary[\"the\"]))"
      ],
      "metadata": {
        "id": "TkeSn0ePqsG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2ac651-8d44-42d1-a509-dff21aa471f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding for 'the' is:\n",
            "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to get a list of embeddings.\n",
        "def get_embeddings(sentence, vocabulary, embedding_size):\n",
        "\n",
        "  # List of embeddings for the input sentence.\n",
        "  embeddings = []\n",
        "\n",
        "  # Retrieving embedding vector for each word.\n",
        "  for word in sentence:\n",
        "\n",
        "    # Computing the embedding.\n",
        "    embedding = vocabulary.get(word.lower())\n",
        "\n",
        "    # Checking the embedding.\n",
        "    if embedding is not None:\n",
        "      \n",
        "      # Populating the list of embeddings.\n",
        "      embeddings.append(embedding)\n",
        "    \n",
        "    else:\n",
        "\n",
        "      # Storing vector of zeros for OOV terms.\n",
        "      embeddings.append(list(np.zeros(embedding_size)))\n",
        "\n",
        "  # Returning list of embeddings.\n",
        "  return embeddings\n",
        "\n",
        "# List containing dataframe rows.\n",
        "dataframe_rows = []\n",
        "\n",
        "# List containing words of a single sentence.\n",
        "row_words = []\n",
        "\n",
        "# List containing tags of a single sentence.\n",
        "row_tags = []\n",
        "\n",
        "# Defining data folder path.\n",
        "folder = os.path.join(data_path, dataset_name)\n",
        "\n",
        "# Storing rows.\n",
        "for filename in sorted(os.listdir(folder)):\n",
        "\n",
        "  # Computing path to file.\n",
        "  file_path = os.path.join(folder, filename)\n",
        "\n",
        "  # Checking existance of file.\n",
        "  if os.path.isfile(file_path):\n",
        "\n",
        "    # Opening the file.\n",
        "    with open(file_path, mode = \"r\") as text_file:\n",
        "\n",
        "      # Reading lines.\n",
        "      while True:\n",
        "\n",
        "        # Reading next line.\n",
        "        line = text_file.readline()\n",
        "\n",
        "        # Checking that line is different from \"\\n\" (empty line) and from last line (EOF).\n",
        "        if line and line != \"\\n\":\n",
        "\n",
        "          # Storing the word.\n",
        "          row_words.append(line.split()[0])\n",
        "\n",
        "          # Storing the POS tag.\n",
        "          row_tags.append(line.split()[1])\n",
        "\n",
        "        # Creating new dataframe row.\n",
        "        else:\n",
        "\n",
        "          # Creating a row.\n",
        "          dataframe_row = {\"file_id\": int(filename.split(\".\")[0].split(\"_\")[1]), \n",
        "                           \"sentence\": row_words, \n",
        "                           \"tags\": row_tags, \n",
        "                           \"features\": get_embeddings(row_words, embedding_vocabulary, EMBEDDING_SIZE)}\n",
        "\n",
        "          # Appending row.\n",
        "          dataframe_rows.append(dataframe_row)\n",
        "\n",
        "          # Resetting row_words list so to store a new sentence.\n",
        "          row_words = []\n",
        "\n",
        "          # Resetting row_tags list so to store a new sentence.\n",
        "          row_tags = []\n",
        "\n",
        "          # If, in particular, EOF is reached, then break the inner loop.\n",
        "          if not line: break\n",
        "\n",
        "# Creating pandas dataframe.\n",
        "dataframe = pd.DataFrame(dataframe_rows)\n",
        "\n",
        "# Printing dataframe head.\n",
        "dataframe.head()"
      ],
      "metadata": {
        "id": "KACvAIENUlQ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9e9760e5-a198-4154-d4e2-de176efd736d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   file_id                                           sentence  \\\n",
              "0        1  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
              "1        1  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
              "2        2  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
              "3        3  [A, form, of, asbestos, once, used, to, make, ...   \n",
              "4        3  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
              "\n",
              "                                                tags  \\\n",
              "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
              "1  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...   \n",
              "2  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
              "3  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
              "4  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...   \n",
              "\n",
              "                                            features  \n",
              "0  [[0.23568, 0.39638, -0.60135, -0.52681, 0.1587...  \n",
              "1  [[0.006008, 0.57028, -0.064426, -0.044687, 0.8...  \n",
              "2  [[0.86274, 0.056588, -0.081828, -0.35318, -0.0...  \n",
              "3  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
              "4  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88e2e133-d05c-4b1f-aaa8-849f7600be7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>tags</th>\n",
              "      <th>features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "      <td>[[0.23568, 0.39638, -0.60135, -0.52681, 0.1587...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
              "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
              "      <td>[[0.006008, 0.57028, -0.064426, -0.044687, 0.8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
              "      <td>[[0.86274, 0.056588, -0.081828, -0.35318, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88e2e133-d05c-4b1f-aaa8-849f7600be7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88e2e133-d05c-4b1f-aaa8-849f7600be7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88e2e133-d05c-4b1f-aaa8-849f7600be7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining training split.\n",
        "TRAINING_SPLIT = range(1, 101)\n",
        "\n",
        "# Defining validation split.\n",
        "VALIDATION_SPLIT = range(101, 151)\n",
        "\n",
        "# Defining test split.\n",
        "TEST_SPLIT = range(151, 200)\n",
        "\n",
        "# Computing train dataframe.\n",
        "train = dataframe.loc[dataframe[\"file_id\"].isin(TRAINING_SPLIT)]\n",
        "\n",
        "# Computing validation dataframe.\n",
        "validation = dataframe.loc[dataframe[\"file_id\"].isin(VALIDATION_SPLIT)]\n",
        "\n",
        "# Computing test dataframe.\n",
        "test = dataframe.loc[dataframe[\"file_id\"].isin(TEST_SPLIT)]"
      ],
      "metadata": {
        "id": "51jxVKeVa0C3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing length of longest train sentence.\n",
        "MAX_LENGTH = len(max(train[\"sentence\"].tolist(), key = len))\n",
        "\n",
        "# Padding train features.\n",
        "train_features = pad_sequences(train[\"features\"].tolist(), maxlen = MAX_LENGTH, padding = \"post\", dtype = \"float32\")\n",
        "\n",
        "# Padding validation features.\n",
        "validation_features = pad_sequences(validation[\"features\"].tolist(), maxlen = MAX_LENGTH, padding = \"post\", dtype = \"float32\")\n",
        "\n",
        "# Padding test features.\n",
        "test_features = pad_sequences(test[\"features\"].tolist(), maxlen = MAX_LENGTH, padding = \"post\", dtype = \"float32\")"
      ],
      "metadata": {
        "id": "jTgbsNPNoI9g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing all possible tags. The training set already contains all of them.\n",
        "tags = [item for sublist in train[\"tags\"].tolist() for item in sublist]\n",
        "\n",
        "# Removing duplicates from tags. By using a dict instead of a set I can get reproducible results (sets are not ordered).\n",
        "tags = list(dict.fromkeys(tags))\n",
        "\n",
        "# Vocabulary for tags.\n",
        "tag_to_index = {}\n",
        "\n",
        "# PAD is mapped onto 0.\n",
        "tag_to_index[\"PAD\"] = 0\n",
        "\n",
        "# All other tags are mapped onto other indexes, starting from 1 up to |tags|.\n",
        "for i, tag in enumerate(list(tags)): tag_to_index[tag] = i + 1\n",
        "\n",
        "# Function used to transform list of tags into vectors of integers using a tag-to-index vocabulary.\n",
        "def convert_tags(input_tags, vocabulary):\n",
        "\n",
        "  # Output tags.\n",
        "  output_tags = []\n",
        "\n",
        "  # Converting input tags.\n",
        "  for tags_list in input_tags:\n",
        "\n",
        "    # Computing index.\n",
        "    output_tags.append([vocabulary[tag] for tag in tags_list])\n",
        "\n",
        "  # Returning output_tags.\n",
        "  return output_tags\n",
        "\n",
        "# Computing train tags.\n",
        "train_tags = convert_tags(input_tags = train[\"tags\"].tolist(), vocabulary = tag_to_index)\n",
        "\n",
        "# Computing validation tags.\n",
        "validation_tags = convert_tags(input_tags = validation[\"tags\"].tolist(), vocabulary = tag_to_index)\n",
        "\n",
        "# Computing test tags.\n",
        "test_tags = convert_tags(input_tags = test[\"tags\"].tolist(), vocabulary = tag_to_index)\n",
        "\n",
        "# Padding train tags.\n",
        "train_tags = pad_sequences(train_tags, maxlen = MAX_LENGTH, padding = \"post\")\n",
        "\n",
        "# Padding validation tags.\n",
        "validation_tags = pad_sequences(validation_tags, maxlen = MAX_LENGTH, padding = \"post\")\n",
        "\n",
        "# Padding test tags.\n",
        "test_tags = pad_sequences(test_tags, maxlen = MAX_LENGTH, padding = \"post\")"
      ],
      "metadata": {
        "id": "gzIGFLjNibLW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Definition and Training"
      ],
      "metadata": {
        "id": "Eefy9o5A6e6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models' names.\n",
        "models_name = [\"m_0\", \"m_1\", \"m_2\", \"m_3\"]\n",
        "\n",
        "# Dictionary of models' description.\n",
        "descriptions_dict = {models_name[0]: (f\"Baseline model ({models_name[0]}): \\n\"\n",
        "                                      \"- Bi-directional LSTM layer. \\n\"\n",
        "                                      \"- Time-distributed dense layer. \\n\"\n",
        "                                      \"- Softmax activation function.\"),\n",
        "                     models_name[1]: (f\"GRU model ({models_name[1]}): \\n\"\n",
        "                                      \"- GRU layer. \\n\"\n",
        "                                      \"- Time-distributed dense layer. \\n\"\n",
        "                                      \"- Softmax activation function.\"),\n",
        "                     models_name[2]: (f\"Double bi-directional LSTM model ({models_name[2]}): \\n\"\n",
        "                                      \"- Bi-directional LSTM layer. \\n\"\n",
        "                                      \"- Bi-directional LSTM layer. \\n\"\n",
        "                                      \"- Time-distributed dense layer. \\n\"\n",
        "                                      \"- Softmax activation function.\"),\n",
        "                     models_name[3]: (f\"Double dense layer model ({models_name[3]}): \\n\"\n",
        "                                      \"- Bi-directional LSTM layer. \\n\"\n",
        "                                      \"- Time-distributed dense layer. \\n\"\n",
        "                                      \"- ReLU activation function. \\n\"\n",
        "                                      \"- Time-distributed dense layer. \\n\"\n",
        "                                      \"- Softmax activation function.\")}\n",
        "\n",
        "# Dictionary of models.\n",
        "models = {}"
      ],
      "metadata": {
        "id": "HFHMaanl2IRf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Epochs.\n",
        "EPOCHS = 100\n",
        "\n",
        "# Initial learning rate.\n",
        "LR = 0.01\n",
        "\n",
        "# Weight decay parameter.\n",
        "REG = 0.01\n",
        "\n",
        "# Early stopping callback.\n",
        "early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 5, restore_best_weights = True)\n",
        "\n",
        "# Reduce learning rate on plateau callback.\n",
        "reduce_lr = ReduceLROnPlateau(monitor = \"val_loss\", patience = 3, factor = 0.1)"
      ],
      "metadata": {
        "id": "5zt4AACHGSNA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that creates the model.\n",
        "def get_model(name, layers, input_shape):\n",
        "\n",
        "  # Sequential model.\n",
        "  model = Sequential()\n",
        "\n",
        "  # Adding input layer.\n",
        "  model.add(InputLayer(input_shape = input_shape))\n",
        "\n",
        "  # Adding layers.\n",
        "  for layer in layers:\n",
        "\n",
        "    # Adding layers.\n",
        "    model.add(layer)\n",
        "  \n",
        "  # Output dense layer.\n",
        "  model.add(TimeDistributed(Dense(len(tag_to_index))))\n",
        "\n",
        "  # Softmax.\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  # Adding a name to the model.\n",
        "  model._name = name\n",
        "\n",
        "  # Returning the model.\n",
        "  return model"
      ],
      "metadata": {
        "id": "UzNmDy4dLmjL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to grid-search.\n",
        "def grid_search(model_name, units, best_baseline_LSTM_units = None):\n",
        "\n",
        "  # List of models obtained during grid-search.\n",
        "  models = []\n",
        "\n",
        "  # List of histories.\n",
        "  histories = []\n",
        "\n",
        "  # Printing description of the model.\n",
        "  print(f\"Grid-search, {model_name} model.\")\n",
        "\n",
        "  # Grid-search over possible number of units so to find the best model.\n",
        "  for n in units:\n",
        "\n",
        "    # Checking model name.\n",
        "    if model_name == \"m_0\":\n",
        "\n",
        "      # List of layers.\n",
        "      layers = [Bidirectional(LSTM(n, return_sequences = True, recurrent_regularizer = l2(REG)))]\n",
        "    \n",
        "    elif model_name == \"m_1\":\n",
        "\n",
        "      # List of layers.\n",
        "      layers = [GRU(n, return_sequences = True, recurrent_regularizer = l2(REG))]\n",
        "\n",
        "    elif model_name == \"m_2\" and best_baseline_LSTM_units != None:\n",
        "\n",
        "      # List of layers.\n",
        "      layers = [Bidirectional(LSTM(best_baseline_LSTM_units, return_sequences = True, recurrent_regularizer = l2(REG))),\n",
        "                Bidirectional(LSTM(n, return_sequences = True, recurrent_regularizer = l2(REG)))]\n",
        "\n",
        "    elif model_name == \"m_3\" and best_baseline_LSTM_units != None:\n",
        "\n",
        "      # List of layers.\n",
        "      layers = [Bidirectional(LSTM(best_baseline_LSTM_units, return_sequences = True, recurrent_regularizer = l2(REG))),\n",
        "                TimeDistributed(Dense(n)),\n",
        "                Activation(\"relu\")]\n",
        "\n",
        "    # Creating the double lstm model.\n",
        "    model = get_model(name = model_name, layers = layers, input_shape = (MAX_LENGTH, EMBEDDING_SIZE))\n",
        "\n",
        "    # Compiling.\n",
        "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = Adam(LR), metrics = [\"accuracy\"])\n",
        "\n",
        "    # Storing model.\n",
        "    models.append(model)\n",
        "\n",
        "    # Printing info.\n",
        "    print(f\"\\nNumber of units: {n}.\\n\")\n",
        "\n",
        "    # Printing summary.\n",
        "    models[-1].summary()\n",
        "\n",
        "    # Fitting the model.\n",
        "    history = models[-1].fit(train_features, train_tags, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (validation_features, validation_tags), callbacks = [early_stopping, reduce_lr])\n",
        "\n",
        "    # Storing history.\n",
        "    histories.append(history)\n",
        "\n",
        "  # Returning models and histories.\n",
        "  return models, histories"
      ],
      "metadata": {
        "id": "_ziwP6lcHh6q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Punctuation tag list.\n",
        "punctuation_tag_list = [\"PAD\", \",\", \".\", \"``\", \"''\", \":\", \"$\", \"#\"]\n",
        "\n",
        "# Function used to compute macro F1-score.\n",
        "def compute_F1_score(model, X, y, tag_to_index_vocabulary):\n",
        "\n",
        "  # Computing predictions.\n",
        "  pred = model.predict(X)\n",
        "\n",
        "  # Computing classification report.\n",
        "  report = classification_report(y.flatten(), \n",
        "                                 np.argmax(pred, axis = 2).flatten(), \n",
        "                                 labels = np.arange(0, len(tag_to_index_vocabulary), 1),\n",
        "                                 target_names = list(tag_to_index_vocabulary.keys()),\n",
        "                                 zero_division = 0,\n",
        "                                 output_dict = True)\n",
        "\n",
        "  # Macro F1-score without punctuation classes.\n",
        "  macro_f1 = 0\n",
        "\n",
        "  # Iterating over classes.\n",
        "  for tag in list(tag_to_index_vocabulary.keys()):\n",
        "\n",
        "    # Updating the macro F1-score.\n",
        "    if tag not in punctuation_tag_list: macro_f1 = macro_f1 + report[tag][\"f1-score\"]\n",
        "\n",
        "  # Dividing macro F1-score with the number of non-punctuation classes.\n",
        "  macro_f1 = macro_f1 / (len(list(tag_to_index_vocabulary.keys())) - len(punctuation_tag_list))\n",
        "\n",
        "  # Returning macro F1-score.\n",
        "  return macro_f1"
      ],
      "metadata": {
        "id": "i1oYG_Kj40Km"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to get the best model among a set of models.\n",
        "def get_best_model(models, units):\n",
        "\n",
        "  # Computing F1-scores.\n",
        "  f1_scores = [compute_F1_score(model, validation_features, validation_tags, tag_to_index) for model in models]\n",
        "\n",
        "  # Computing best number of units .\n",
        "  best = units[np.argmax(f1_scores)]\n",
        "\n",
        "  # Printing best number of units.\n",
        "  print(f\"The best number of units is: {best}.\")\n",
        "\n",
        "  # Returning the best model.\n",
        "  return best, f1_scores, models[np.argmax(f1_scores)]"
      ],
      "metadata": {
        "id": "O1R8c9swIzIA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to plot training losses.\n",
        "def plot_training_loss(model_name, units, histories):\n",
        "\n",
        "  # Computing best epoch for each model.\n",
        "  best_epochs = [np.argmin(history.history[\"val_loss\"]) + 1 for history in histories]\n",
        "\n",
        "  # Creating the figure and axes.\n",
        "  fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "\n",
        "  # Plotting every history.\n",
        "  for i in range(len(units)):\n",
        "\n",
        "    # Computing the x axis array.\n",
        "    x = np.arange(1, len(histories[i].history[\"val_loss\"]) + 1, 1)\n",
        "\n",
        "    # Plotting validation loss.\n",
        "    ax.plot(x, histories[i].history[\"val_loss\"], label = r\"${}^{}$\".format(model_name, i + 1))\n",
        "\n",
        "  # Setting labels.\n",
        "  ax.set_ylabel(\"Loss\")\n",
        "\n",
        "  # Setting labels.\n",
        "  ax.set_xlabel(\"Epoch\")\n",
        "\n",
        "  # Displying legend.\n",
        "  ax.legend()\n",
        "\n",
        "  # Defining figures folder path.\n",
        "  figures_path = os.path.join(os.getcwd(), \"figures\")\n",
        "\n",
        "  # Creating data folder.\n",
        "  if not os.path.exists(figures_path): os.makedirs(figures_path)\n",
        "\n",
        "  # Saving the figure.\n",
        "  fig.savefig(f\"{figures_path}/val_loss_{model_name}.pdf\", bbox_inches = \"tight\")\n",
        "\n",
        "  # Showing the plot.\n",
        "  plt.show\n",
        "\n",
        "# Function used to plot f1_scores.\n",
        "def plot_f1_scores(model_name, units, f1_scores):\n",
        "  \n",
        "  # Creating the figure and axes.\n",
        "  fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "\n",
        "  # Plotting.\n",
        "  ax.plot(np.arange(1, len(f1_scores) + 1, 1), f1_scores, marker = \"o\")\n",
        "\n",
        "  # Setting labels.\n",
        "  ax.set_ylabel(r\"$F_1$-score\")\n",
        "\n",
        "  # Setting labels.\n",
        "  ax.set_xlabel(\"Model\")\n",
        "\n",
        "  # Setting x ticks labels.\n",
        "  ax.set_xticklabels([r\"${}^{}$\".format(model_name, i) for i in range(len(units) + 1)])\n",
        "\n",
        "  # Defining figures folder path.\n",
        "  figures_path = os.path.join(os.getcwd(), \"figures\")\n",
        "\n",
        "  # Creating data folder.\n",
        "  if not os.path.exists(figures_path): os.makedirs(figures_path)\n",
        "\n",
        "  # Saving the figure.\n",
        "  fig.savefig(f\"{figures_path}/f1_scores_{model_name}.pdf\", bbox_inches = \"tight\")\n",
        "\n",
        "  # Showing the plot.\n",
        "  plt.show"
      ],
      "metadata": {
        "id": "Ti49IGGKJPpG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model ($m_0$)"
      ],
      "metadata": {
        "id": "t4YdLEyuxPuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible units.\n",
        "baseline_units = [32, 64, 128, 256]\n",
        "\n",
        "# Computing models and histories.\n",
        "baseline_models, baseline_model_histories = grid_search(models_name[0], baseline_units)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "momN-VqKCtlx",
        "outputId": "c3a5f55f-d95b-40b9-f866-fc9bcfd918d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid-search, m_0 model.\n",
            "\n",
            "Number of units: 32.\n",
            "\n",
            "Model: \"m_0\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirectiona  (None, 249, 64)          21248     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 249, 46)          2990      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,238\n",
            "Trainable params: 24,238\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 9s 104ms/step - loss: 1.8719 - accuracy: 0.8906 - val_loss: 0.5530 - val_accuracy: 0.9305 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.4803 - accuracy: 0.9394 - val_loss: 0.3939 - val_accuracy: 0.9465 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.3333 - accuracy: 0.9502 - val_loss: 0.2716 - val_accuracy: 0.9550 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.2343 - accuracy: 0.9577 - val_loss: 0.2005 - val_accuracy: 0.9603 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.1766 - accuracy: 0.9643 - val_loss: 0.1596 - val_accuracy: 0.9660 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.1433 - accuracy: 0.9693 - val_loss: 0.1351 - val_accuracy: 0.9697 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.1225 - accuracy: 0.9725 - val_loss: 0.1192 - val_accuracy: 0.9720 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.1083 - accuracy: 0.9745 - val_loss: 0.1078 - val_accuracy: 0.9737 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0977 - accuracy: 0.9764 - val_loss: 0.0988 - val_accuracy: 0.9754 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0892 - accuracy: 0.9780 - val_loss: 0.0917 - val_accuracy: 0.9768 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0824 - accuracy: 0.9794 - val_loss: 0.0857 - val_accuracy: 0.9778 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0765 - accuracy: 0.9805 - val_loss: 0.0807 - val_accuracy: 0.9790 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0715 - accuracy: 0.9815 - val_loss: 0.0766 - val_accuracy: 0.9798 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0672 - accuracy: 0.9827 - val_loss: 0.0730 - val_accuracy: 0.9808 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0635 - accuracy: 0.9835 - val_loss: 0.0699 - val_accuracy: 0.9816 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0602 - accuracy: 0.9843 - val_loss: 0.0672 - val_accuracy: 0.9823 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0572 - accuracy: 0.9852 - val_loss: 0.0648 - val_accuracy: 0.9829 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0546 - accuracy: 0.9858 - val_loss: 0.0627 - val_accuracy: 0.9834 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0523 - accuracy: 0.9863 - val_loss: 0.0609 - val_accuracy: 0.9838 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0502 - accuracy: 0.9868 - val_loss: 0.0594 - val_accuracy: 0.9840 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0483 - accuracy: 0.9872 - val_loss: 0.0579 - val_accuracy: 0.9843 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0466 - accuracy: 0.9876 - val_loss: 0.0566 - val_accuracy: 0.9846 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0450 - accuracy: 0.9880 - val_loss: 0.0555 - val_accuracy: 0.9850 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0436 - accuracy: 0.9883 - val_loss: 0.0546 - val_accuracy: 0.9850 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0422 - accuracy: 0.9886 - val_loss: 0.0535 - val_accuracy: 0.9853 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0409 - accuracy: 0.9889 - val_loss: 0.0529 - val_accuracy: 0.9855 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0397 - accuracy: 0.9892 - val_loss: 0.0522 - val_accuracy: 0.9856 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0386 - accuracy: 0.9895 - val_loss: 0.0513 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0377 - accuracy: 0.9897 - val_loss: 0.0509 - val_accuracy: 0.9858 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0367 - accuracy: 0.9900 - val_loss: 0.0504 - val_accuracy: 0.9859 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0358 - accuracy: 0.9903 - val_loss: 0.0498 - val_accuracy: 0.9862 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 0.0493 - val_accuracy: 0.9863 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0340 - accuracy: 0.9907 - val_loss: 0.0489 - val_accuracy: 0.9863 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0332 - accuracy: 0.9909 - val_loss: 0.0485 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0325 - accuracy: 0.9911 - val_loss: 0.0482 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0318 - accuracy: 0.9913 - val_loss: 0.0478 - val_accuracy: 0.9866 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0311 - accuracy: 0.9915 - val_loss: 0.0476 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.0472 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0297 - accuracy: 0.9919 - val_loss: 0.0470 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0291 - accuracy: 0.9920 - val_loss: 0.0468 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0285 - accuracy: 0.9921 - val_loss: 0.0465 - val_accuracy: 0.9870 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0280 - accuracy: 0.9923 - val_loss: 0.0463 - val_accuracy: 0.9870 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0274 - accuracy: 0.9925 - val_loss: 0.0462 - val_accuracy: 0.9870 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0268 - accuracy: 0.9926 - val_loss: 0.0461 - val_accuracy: 0.9870 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0264 - accuracy: 0.9928 - val_loss: 0.0459 - val_accuracy: 0.9871 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0259 - accuracy: 0.9929 - val_loss: 0.0458 - val_accuracy: 0.9872 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0254 - accuracy: 0.9930 - val_loss: 0.0457 - val_accuracy: 0.9872 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0249 - accuracy: 0.9932 - val_loss: 0.0456 - val_accuracy: 0.9873 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0245 - accuracy: 0.9933 - val_loss: 0.0454 - val_accuracy: 0.9873 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0241 - accuracy: 0.9934 - val_loss: 0.0454 - val_accuracy: 0.9873 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0237 - accuracy: 0.9936 - val_loss: 0.0453 - val_accuracy: 0.9873 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0232 - accuracy: 0.9937 - val_loss: 0.0453 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0230 - accuracy: 0.9938 - val_loss: 0.0453 - val_accuracy: 0.9873 - lr: 0.0100\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0225 - accuracy: 0.9938 - val_loss: 0.0450 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0222 - accuracy: 0.9940 - val_loss: 0.0452 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0219 - accuracy: 0.9940 - val_loss: 0.0451 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0215 - accuracy: 0.9942 - val_loss: 0.0453 - val_accuracy: 0.9875 - lr: 0.0100\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0209 - accuracy: 0.9945 - val_loss: 0.0450 - val_accuracy: 0.9875 - lr: 1.0000e-03\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0207 - accuracy: 0.9945 - val_loss: 0.0449 - val_accuracy: 0.9876 - lr: 1.0000e-03\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0207 - accuracy: 0.9945 - val_loss: 0.0449 - val_accuracy: 0.9875 - lr: 1.0000e-03\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0206 - accuracy: 0.9945 - val_loss: 0.0449 - val_accuracy: 0.9875 - lr: 1.0000e-04\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0206 - accuracy: 0.9945 - val_loss: 0.0449 - val_accuracy: 0.9876 - lr: 1.0000e-04\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0206 - accuracy: 0.9945 - val_loss: 0.0449 - val_accuracy: 0.9875 - lr: 1.0000e-04\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0206 - accuracy: 0.9945 - val_loss: 0.0449 - val_accuracy: 0.9875 - lr: 1.0000e-05\n",
            "\n",
            "Number of units: 64.\n",
            "\n",
            "Model: \"m_0\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_1 (Bidirectio  (None, 249, 128)         58880     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 249, 46)          5934      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 64,814\n",
            "Trainable params: 64,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 5s 104ms/step - loss: 1.6377 - accuracy: 0.9001 - val_loss: 0.6213 - val_accuracy: 0.9400 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.4928 - accuracy: 0.9457 - val_loss: 0.3520 - val_accuracy: 0.9524 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.2754 - accuracy: 0.9582 - val_loss: 0.2077 - val_accuracy: 0.9632 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.1713 - accuracy: 0.9678 - val_loss: 0.1438 - val_accuracy: 0.9696 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.1254 - accuracy: 0.9722 - val_loss: 0.1150 - val_accuracy: 0.9727 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.1026 - accuracy: 0.9755 - val_loss: 0.0984 - val_accuracy: 0.9762 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0881 - accuracy: 0.9785 - val_loss: 0.0871 - val_accuracy: 0.9779 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0775 - accuracy: 0.9806 - val_loss: 0.0789 - val_accuracy: 0.9792 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0695 - accuracy: 0.9822 - val_loss: 0.0724 - val_accuracy: 0.9806 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0628 - accuracy: 0.9838 - val_loss: 0.0671 - val_accuracy: 0.9822 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0575 - accuracy: 0.9851 - val_loss: 0.0629 - val_accuracy: 0.9831 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0531 - accuracy: 0.9861 - val_loss: 0.0593 - val_accuracy: 0.9837 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0492 - accuracy: 0.9870 - val_loss: 0.0563 - val_accuracy: 0.9845 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0459 - accuracy: 0.9879 - val_loss: 0.0540 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0430 - accuracy: 0.9884 - val_loss: 0.0517 - val_accuracy: 0.9858 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0404 - accuracy: 0.9892 - val_loss: 0.0500 - val_accuracy: 0.9862 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0382 - accuracy: 0.9897 - val_loss: 0.0485 - val_accuracy: 0.9864 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0362 - accuracy: 0.9903 - val_loss: 0.0471 - val_accuracy: 0.9868 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0344 - accuracy: 0.9907 - val_loss: 0.0461 - val_accuracy: 0.9869 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0328 - accuracy: 0.9911 - val_loss: 0.0450 - val_accuracy: 0.9873 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0312 - accuracy: 0.9916 - val_loss: 0.0443 - val_accuracy: 0.9876 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0299 - accuracy: 0.9920 - val_loss: 0.0434 - val_accuracy: 0.9877 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0286 - accuracy: 0.9923 - val_loss: 0.0427 - val_accuracy: 0.9880 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.0274 - accuracy: 0.9927 - val_loss: 0.0423 - val_accuracy: 0.9881 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.0263 - accuracy: 0.9931 - val_loss: 0.0418 - val_accuracy: 0.9881 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.0253 - accuracy: 0.9933 - val_loss: 0.0413 - val_accuracy: 0.9883 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0243 - accuracy: 0.9935 - val_loss: 0.0412 - val_accuracy: 0.9882 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0234 - accuracy: 0.9938 - val_loss: 0.0404 - val_accuracy: 0.9885 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0226 - accuracy: 0.9940 - val_loss: 0.0405 - val_accuracy: 0.9885 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0217 - accuracy: 0.9942 - val_loss: 0.0401 - val_accuracy: 0.9886 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0209 - accuracy: 0.9945 - val_loss: 0.0398 - val_accuracy: 0.9887 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.0399 - val_accuracy: 0.9888 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.0195 - accuracy: 0.9949 - val_loss: 0.0396 - val_accuracy: 0.9888 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0188 - accuracy: 0.9952 - val_loss: 0.0393 - val_accuracy: 0.9889 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0182 - accuracy: 0.9953 - val_loss: 0.0395 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0177 - accuracy: 0.9955 - val_loss: 0.0394 - val_accuracy: 0.9889 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0170 - accuracy: 0.9957 - val_loss: 0.0392 - val_accuracy: 0.9891 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0164 - accuracy: 0.9959 - val_loss: 0.0394 - val_accuracy: 0.9889 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.0159 - accuracy: 0.9960 - val_loss: 0.0393 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.0393 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.0145 - accuracy: 0.9964 - val_loss: 0.0392 - val_accuracy: 0.9891 - lr: 1.0000e-03\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-03\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-03\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0142 - accuracy: 0.9965 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-03\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-03\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 1.0000e-05\n",
            "\n",
            "Number of units: 128.\n",
            "\n",
            "Model: \"m_0\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_2 (Bidirectio  (None, 249, 256)         183296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 249, 46)          11822     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 195,118\n",
            "Trainable params: 195,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 5s 118ms/step - loss: 1.7563 - accuracy: 0.9058 - val_loss: 0.7155 - val_accuracy: 0.9500 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.5331 - accuracy: 0.9574 - val_loss: 0.3544 - val_accuracy: 0.9637 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.2664 - accuracy: 0.9697 - val_loss: 0.1914 - val_accuracy: 0.9716 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 0.1500 - accuracy: 0.9752 - val_loss: 0.1204 - val_accuracy: 0.9760 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0993 - accuracy: 0.9788 - val_loss: 0.0899 - val_accuracy: 0.9790 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 0.0762 - accuracy: 0.9818 - val_loss: 0.0745 - val_accuracy: 0.9811 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0632 - accuracy: 0.9842 - val_loss: 0.0655 - val_accuracy: 0.9829 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0544 - accuracy: 0.9860 - val_loss: 0.0593 - val_accuracy: 0.9839 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0481 - accuracy: 0.9873 - val_loss: 0.0545 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0427 - accuracy: 0.9886 - val_loss: 0.0504 - val_accuracy: 0.9858 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0387 - accuracy: 0.9895 - val_loss: 0.0479 - val_accuracy: 0.9863 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0353 - accuracy: 0.9904 - val_loss: 0.0455 - val_accuracy: 0.9870 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0326 - accuracy: 0.9912 - val_loss: 0.0438 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0300 - accuracy: 0.9920 - val_loss: 0.0424 - val_accuracy: 0.9878 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 0.0278 - accuracy: 0.9926 - val_loss: 0.0409 - val_accuracy: 0.9882 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0260 - accuracy: 0.9931 - val_loss: 0.0404 - val_accuracy: 0.9883 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0242 - accuracy: 0.9937 - val_loss: 0.0393 - val_accuracy: 0.9888 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 0.0226 - accuracy: 0.9941 - val_loss: 0.0390 - val_accuracy: 0.9888 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0213 - accuracy: 0.9945 - val_loss: 0.0384 - val_accuracy: 0.9891 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0200 - accuracy: 0.9949 - val_loss: 0.0384 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0189 - accuracy: 0.9952 - val_loss: 0.0376 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0177 - accuracy: 0.9956 - val_loss: 0.0382 - val_accuracy: 0.9892 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0167 - accuracy: 0.9959 - val_loss: 0.0376 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0157 - accuracy: 0.9961 - val_loss: 0.0374 - val_accuracy: 0.9894 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0147 - accuracy: 0.9965 - val_loss: 0.0371 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0139 - accuracy: 0.9967 - val_loss: 0.0370 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0132 - accuracy: 0.9968 - val_loss: 0.0370 - val_accuracy: 0.9896 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0124 - accuracy: 0.9971 - val_loss: 0.0370 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0118 - accuracy: 0.9973 - val_loss: 0.0373 - val_accuracy: 0.9896 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.0106 - accuracy: 0.9977 - val_loss: 0.0364 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0101 - accuracy: 0.9978 - val_loss: 0.0362 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0100 - accuracy: 0.9978 - val_loss: 0.0362 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0099 - accuracy: 0.9978 - val_loss: 0.0363 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0098 - accuracy: 0.9979 - val_loss: 0.0363 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0097 - accuracy: 0.9979 - val_loss: 0.0363 - val_accuracy: 0.9897 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0097 - accuracy: 0.9979 - val_loss: 0.0363 - val_accuracy: 0.9897 - lr: 1.0000e-04\n",
            "\n",
            "Number of units: 256.\n",
            "\n",
            "Model: \"m_0\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_3 (Bidirectio  (None, 249, 512)         628736    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 249, 46)          23598     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 652,334\n",
            "Trainable params: 652,334\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 6s 154ms/step - loss: 2.0510 - accuracy: 0.9057 - val_loss: 0.7254 - val_accuracy: 0.9529 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 2s 99ms/step - loss: 0.4362 - accuracy: 0.9615 - val_loss: 0.2385 - val_accuracy: 0.9672 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.1671 - accuracy: 0.9725 - val_loss: 0.1188 - val_accuracy: 0.9741 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0943 - accuracy: 0.9782 - val_loss: 0.0824 - val_accuracy: 0.9787 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0691 - accuracy: 0.9822 - val_loss: 0.0675 - val_accuracy: 0.9818 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0567 - accuracy: 0.9849 - val_loss: 0.0590 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0491 - accuracy: 0.9867 - val_loss: 0.0537 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0431 - accuracy: 0.9883 - val_loss: 0.0496 - val_accuracy: 0.9859 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.0384 - accuracy: 0.9895 - val_loss: 0.0467 - val_accuracy: 0.9869 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0345 - accuracy: 0.9906 - val_loss: 0.0437 - val_accuracy: 0.9873 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0317 - accuracy: 0.9914 - val_loss: 0.0425 - val_accuracy: 0.9877 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0289 - accuracy: 0.9922 - val_loss: 0.0397 - val_accuracy: 0.9885 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.0268 - accuracy: 0.9929 - val_loss: 0.0394 - val_accuracy: 0.9886 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0245 - accuracy: 0.9936 - val_loss: 0.0380 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0223 - accuracy: 0.9942 - val_loss: 0.0368 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0205 - accuracy: 0.9948 - val_loss: 0.0356 - val_accuracy: 0.9898 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.0191 - accuracy: 0.9952 - val_loss: 0.0352 - val_accuracy: 0.9898 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0177 - accuracy: 0.9957 - val_loss: 0.0344 - val_accuracy: 0.9901 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0163 - accuracy: 0.9961 - val_loss: 0.0350 - val_accuracy: 0.9900 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0150 - accuracy: 0.9965 - val_loss: 0.0343 - val_accuracy: 0.9902 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.0340 - val_accuracy: 0.9904 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.0335 - val_accuracy: 0.9905 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.0120 - accuracy: 0.9973 - val_loss: 0.0341 - val_accuracy: 0.9905 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.0333 - val_accuracy: 0.9907 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0103 - accuracy: 0.9979 - val_loss: 0.0345 - val_accuracy: 0.9906 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0098 - accuracy: 0.9980 - val_loss: 0.0336 - val_accuracy: 0.9907 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0092 - accuracy: 0.9982 - val_loss: 0.0339 - val_accuracy: 0.9905 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.0076 - accuracy: 0.9986 - val_loss: 0.0325 - val_accuracy: 0.9908 - lr: 1.0000e-03\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0324 - val_accuracy: 0.9908 - lr: 1.0000e-03\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.0325 - val_accuracy: 0.9909 - lr: 1.0000e-03\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0324 - val_accuracy: 0.9909 - lr: 1.0000e-03\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0324 - val_accuracy: 0.9909 - lr: 1.0000e-03\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.0325 - val_accuracy: 0.9909 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.0325 - val_accuracy: 0.9909 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing best model.\n",
        "baseline_best_units, baseline_f1_scores, models[models_name[0]] = get_best_model(baseline_models, baseline_units)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i68Sx5VvShFD",
        "outputId": "6a5f509b-d3cd-4e8e-a5fa-b6c14c41c579"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 1s 9ms/step\n",
            "41/41 [==============================] - 1s 9ms/step\n",
            "41/41 [==============================] - 1s 11ms/step\n",
            "41/41 [==============================] - 2s 18ms/step\n",
            "The best number of units is: 256.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_training_loss(models_name[0], baseline_units, baseline_model_histories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "vexOylzGJJE-",
        "outputId": "30bc985a-708e-4205-b92b-83c5388235c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting scores - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_f1_scores(models_name[0], baseline_units, baseline_f1_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "emXLTTjTgy8_",
        "outputId": "fa908dc6-17de-4937-d562-36a9bee8c435"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU Model ($m_1$)"
      ],
      "metadata": {
        "id": "jPtlS40Jx9gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible units.\n",
        "gru_units = [32, 64, 128, 256]\n",
        "\n",
        "# Computing models and histories.\n",
        "gru_models, gru_model_histories = grid_search(models_name[1], gru_units)"
      ],
      "metadata": {
        "id": "G3gJ7IKwOgHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7a8f67-56d2-468e-a402-b201185f486f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid-search, m_1 model.\n",
            "\n",
            "Number of units: 32.\n",
            "\n",
            "Model: \"m_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 249, 32)           8064      \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, 249, 46)          1518      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,582\n",
            "Trainable params: 9,582\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 3s 64ms/step - loss: 2.0736 - accuracy: 0.9044 - val_loss: 0.4863 - val_accuracy: 0.9193 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.4664 - accuracy: 0.9268 - val_loss: 0.4211 - val_accuracy: 0.9397 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.3698 - accuracy: 0.9434 - val_loss: 0.3080 - val_accuracy: 0.9519 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.2681 - accuracy: 0.9545 - val_loss: 0.2273 - val_accuracy: 0.9588 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2017 - accuracy: 0.9620 - val_loss: 0.1795 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1637 - accuracy: 0.9667 - val_loss: 0.1531 - val_accuracy: 0.9672 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.1421 - accuracy: 0.9696 - val_loss: 0.1368 - val_accuracy: 0.9696 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1277 - accuracy: 0.9718 - val_loss: 0.1254 - val_accuracy: 0.9716 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1169 - accuracy: 0.9735 - val_loss: 0.1164 - val_accuracy: 0.9732 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1083 - accuracy: 0.9748 - val_loss: 0.1095 - val_accuracy: 0.9739 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1014 - accuracy: 0.9759 - val_loss: 0.1036 - val_accuracy: 0.9746 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0956 - accuracy: 0.9768 - val_loss: 0.0990 - val_accuracy: 0.9753 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0907 - accuracy: 0.9775 - val_loss: 0.0948 - val_accuracy: 0.9760 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0864 - accuracy: 0.9782 - val_loss: 0.0913 - val_accuracy: 0.9763 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0827 - accuracy: 0.9789 - val_loss: 0.0882 - val_accuracy: 0.9771 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0794 - accuracy: 0.9796 - val_loss: 0.0855 - val_accuracy: 0.9776 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0765 - accuracy: 0.9803 - val_loss: 0.0831 - val_accuracy: 0.9781 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0739 - accuracy: 0.9806 - val_loss: 0.0810 - val_accuracy: 0.9783 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0717 - accuracy: 0.9810 - val_loss: 0.0791 - val_accuracy: 0.9787 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0696 - accuracy: 0.9814 - val_loss: 0.0775 - val_accuracy: 0.9791 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0677 - accuracy: 0.9819 - val_loss: 0.0760 - val_accuracy: 0.9796 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0661 - accuracy: 0.9822 - val_loss: 0.0748 - val_accuracy: 0.9798 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0646 - accuracy: 0.9825 - val_loss: 0.0735 - val_accuracy: 0.9799 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0632 - accuracy: 0.9828 - val_loss: 0.0725 - val_accuracy: 0.9802 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0619 - accuracy: 0.9831 - val_loss: 0.0715 - val_accuracy: 0.9805 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0608 - accuracy: 0.9834 - val_loss: 0.0707 - val_accuracy: 0.9807 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0597 - accuracy: 0.9836 - val_loss: 0.0699 - val_accuracy: 0.9809 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0587 - accuracy: 0.9839 - val_loss: 0.0690 - val_accuracy: 0.9811 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0577 - accuracy: 0.9840 - val_loss: 0.0685 - val_accuracy: 0.9812 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0568 - accuracy: 0.9842 - val_loss: 0.0679 - val_accuracy: 0.9813 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0560 - accuracy: 0.9844 - val_loss: 0.0672 - val_accuracy: 0.9814 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0553 - accuracy: 0.9845 - val_loss: 0.0667 - val_accuracy: 0.9814 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0546 - accuracy: 0.9847 - val_loss: 0.0662 - val_accuracy: 0.9814 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0538 - accuracy: 0.9848 - val_loss: 0.0658 - val_accuracy: 0.9815 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0532 - accuracy: 0.9850 - val_loss: 0.0653 - val_accuracy: 0.9816 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0526 - accuracy: 0.9852 - val_loss: 0.0649 - val_accuracy: 0.9817 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0521 - accuracy: 0.9852 - val_loss: 0.0647 - val_accuracy: 0.9819 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0516 - accuracy: 0.9853 - val_loss: 0.0642 - val_accuracy: 0.9820 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0511 - accuracy: 0.9855 - val_loss: 0.0639 - val_accuracy: 0.9820 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0506 - accuracy: 0.9856 - val_loss: 0.0636 - val_accuracy: 0.9821 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0502 - accuracy: 0.9857 - val_loss: 0.0632 - val_accuracy: 0.9823 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0498 - accuracy: 0.9859 - val_loss: 0.0630 - val_accuracy: 0.9823 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0493 - accuracy: 0.9859 - val_loss: 0.0628 - val_accuracy: 0.9824 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0489 - accuracy: 0.9860 - val_loss: 0.0626 - val_accuracy: 0.9825 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0485 - accuracy: 0.9861 - val_loss: 0.0623 - val_accuracy: 0.9824 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0482 - accuracy: 0.9863 - val_loss: 0.0620 - val_accuracy: 0.9825 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0479 - accuracy: 0.9863 - val_loss: 0.0619 - val_accuracy: 0.9826 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0474 - accuracy: 0.9865 - val_loss: 0.0616 - val_accuracy: 0.9827 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0471 - accuracy: 0.9865 - val_loss: 0.0615 - val_accuracy: 0.9826 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0468 - accuracy: 0.9866 - val_loss: 0.0612 - val_accuracy: 0.9828 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0465 - accuracy: 0.9867 - val_loss: 0.0613 - val_accuracy: 0.9827 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0462 - accuracy: 0.9867 - val_loss: 0.0610 - val_accuracy: 0.9826 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0459 - accuracy: 0.9868 - val_loss: 0.0608 - val_accuracy: 0.9828 - lr: 0.0100\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0456 - accuracy: 0.9868 - val_loss: 0.0607 - val_accuracy: 0.9828 - lr: 0.0100\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0454 - accuracy: 0.9870 - val_loss: 0.0607 - val_accuracy: 0.9829 - lr: 0.0100\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0452 - accuracy: 0.9870 - val_loss: 0.0603 - val_accuracy: 0.9829 - lr: 0.0100\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0449 - accuracy: 0.9871 - val_loss: 0.0604 - val_accuracy: 0.9830 - lr: 0.0100\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0446 - accuracy: 0.9872 - val_loss: 0.0602 - val_accuracy: 0.9830 - lr: 0.0100\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0601 - val_accuracy: 0.9831 - lr: 0.0100\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0442 - accuracy: 0.9873 - val_loss: 0.0600 - val_accuracy: 0.9831 - lr: 0.0100\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0440 - accuracy: 0.9873 - val_loss: 0.0599 - val_accuracy: 0.9832 - lr: 0.0100\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0437 - accuracy: 0.9874 - val_loss: 0.0598 - val_accuracy: 0.9832 - lr: 0.0100\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0435 - accuracy: 0.9875 - val_loss: 0.0599 - val_accuracy: 0.9832 - lr: 0.0100\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0433 - accuracy: 0.9875 - val_loss: 0.0597 - val_accuracy: 0.9834 - lr: 0.0100\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0431 - accuracy: 0.9875 - val_loss: 0.0597 - val_accuracy: 0.9833 - lr: 0.0100\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0430 - accuracy: 0.9876 - val_loss: 0.0595 - val_accuracy: 0.9834 - lr: 0.0100\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0427 - accuracy: 0.9876 - val_loss: 0.0594 - val_accuracy: 0.9835 - lr: 0.0100\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0426 - accuracy: 0.9878 - val_loss: 0.0594 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0423 - accuracy: 0.9878 - val_loss: 0.0595 - val_accuracy: 0.9834 - lr: 0.0100\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0422 - accuracy: 0.9877 - val_loss: 0.0593 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0420 - accuracy: 0.9879 - val_loss: 0.0591 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0418 - accuracy: 0.9879 - val_loss: 0.0591 - val_accuracy: 0.9837 - lr: 0.0100\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0417 - accuracy: 0.9880 - val_loss: 0.0590 - val_accuracy: 0.9837 - lr: 0.0100\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0415 - accuracy: 0.9880 - val_loss: 0.0589 - val_accuracy: 0.9838 - lr: 0.0100\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0413 - accuracy: 0.9880 - val_loss: 0.0590 - val_accuracy: 0.9838 - lr: 0.0100\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0412 - accuracy: 0.9880 - val_loss: 0.0589 - val_accuracy: 0.9838 - lr: 0.0100\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0406 - accuracy: 0.9883 - val_loss: 0.0587 - val_accuracy: 0.9838 - lr: 1.0000e-03\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0405 - accuracy: 0.9883 - val_loss: 0.0587 - val_accuracy: 0.9839 - lr: 1.0000e-03\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0405 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-03\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0404 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-03\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0404 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-04\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0404 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-04\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0404 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-04\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-05\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-05\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-05\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-06\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-06\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-06\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-07\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-07\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-07\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0586 - val_accuracy: 0.9839 - lr: 1.0000e-08\n",
            "\n",
            "Number of units: 64.\n",
            "\n",
            "Model: \"m_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_1 (GRU)                 (None, 249, 64)           22272     \n",
            "                                                                 \n",
            " time_distributed_5 (TimeDis  (None, 249, 46)          2990      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,262\n",
            "Trainable params: 25,262\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 3s 67ms/step - loss: 1.6647 - accuracy: 0.9079 - val_loss: 0.5457 - val_accuracy: 0.9350 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.4820 - accuracy: 0.9411 - val_loss: 0.3813 - val_accuracy: 0.9468 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.3122 - accuracy: 0.9530 - val_loss: 0.2421 - val_accuracy: 0.9590 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.2033 - accuracy: 0.9632 - val_loss: 0.1713 - val_accuracy: 0.9654 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1530 - accuracy: 0.9675 - val_loss: 0.1413 - val_accuracy: 0.9680 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.1298 - accuracy: 0.9703 - val_loss: 0.1250 - val_accuracy: 0.9702 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.1156 - accuracy: 0.9726 - val_loss: 0.1135 - val_accuracy: 0.9728 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.1050 - accuracy: 0.9747 - val_loss: 0.1048 - val_accuracy: 0.9741 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0966 - accuracy: 0.9761 - val_loss: 0.0977 - val_accuracy: 0.9751 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0896 - accuracy: 0.9773 - val_loss: 0.0918 - val_accuracy: 0.9760 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0836 - accuracy: 0.9784 - val_loss: 0.0867 - val_accuracy: 0.9768 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0784 - accuracy: 0.9794 - val_loss: 0.0823 - val_accuracy: 0.9779 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0739 - accuracy: 0.9804 - val_loss: 0.0786 - val_accuracy: 0.9787 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0700 - accuracy: 0.9814 - val_loss: 0.0754 - val_accuracy: 0.9794 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0667 - accuracy: 0.9822 - val_loss: 0.0727 - val_accuracy: 0.9802 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0638 - accuracy: 0.9828 - val_loss: 0.0705 - val_accuracy: 0.9808 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0612 - accuracy: 0.9835 - val_loss: 0.0684 - val_accuracy: 0.9812 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0589 - accuracy: 0.9840 - val_loss: 0.0666 - val_accuracy: 0.9816 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0569 - accuracy: 0.9846 - val_loss: 0.0652 - val_accuracy: 0.9822 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0550 - accuracy: 0.9851 - val_loss: 0.0639 - val_accuracy: 0.9826 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0533 - accuracy: 0.9855 - val_loss: 0.0626 - val_accuracy: 0.9828 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0518 - accuracy: 0.9857 - val_loss: 0.0615 - val_accuracy: 0.9830 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0503 - accuracy: 0.9861 - val_loss: 0.0605 - val_accuracy: 0.9832 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0490 - accuracy: 0.9865 - val_loss: 0.0597 - val_accuracy: 0.9834 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0478 - accuracy: 0.9867 - val_loss: 0.0588 - val_accuracy: 0.9835 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0467 - accuracy: 0.9870 - val_loss: 0.0583 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0456 - accuracy: 0.9873 - val_loss: 0.0575 - val_accuracy: 0.9838 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0446 - accuracy: 0.9876 - val_loss: 0.0567 - val_accuracy: 0.9841 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0437 - accuracy: 0.9878 - val_loss: 0.0564 - val_accuracy: 0.9842 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0428 - accuracy: 0.9879 - val_loss: 0.0562 - val_accuracy: 0.9842 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0421 - accuracy: 0.9882 - val_loss: 0.0555 - val_accuracy: 0.9844 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0412 - accuracy: 0.9884 - val_loss: 0.0549 - val_accuracy: 0.9845 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0404 - accuracy: 0.9887 - val_loss: 0.0546 - val_accuracy: 0.9847 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0397 - accuracy: 0.9888 - val_loss: 0.0543 - val_accuracy: 0.9848 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0391 - accuracy: 0.9890 - val_loss: 0.0541 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0385 - accuracy: 0.9891 - val_loss: 0.0537 - val_accuracy: 0.9850 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0379 - accuracy: 0.9893 - val_loss: 0.0535 - val_accuracy: 0.9851 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0372 - accuracy: 0.9894 - val_loss: 0.0533 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0367 - accuracy: 0.9896 - val_loss: 0.0530 - val_accuracy: 0.9851 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0362 - accuracy: 0.9897 - val_loss: 0.0528 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0356 - accuracy: 0.9898 - val_loss: 0.0527 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0352 - accuracy: 0.9899 - val_loss: 0.0524 - val_accuracy: 0.9853 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0348 - accuracy: 0.9901 - val_loss: 0.0522 - val_accuracy: 0.9854 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0342 - accuracy: 0.9902 - val_loss: 0.0524 - val_accuracy: 0.9854 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0338 - accuracy: 0.9903 - val_loss: 0.0522 - val_accuracy: 0.9855 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0334 - accuracy: 0.9903 - val_loss: 0.0520 - val_accuracy: 0.9855 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0330 - accuracy: 0.9906 - val_loss: 0.0520 - val_accuracy: 0.9856 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0326 - accuracy: 0.9906 - val_loss: 0.0518 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 0.0518 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0319 - accuracy: 0.9907 - val_loss: 0.0519 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0316 - accuracy: 0.9909 - val_loss: 0.0516 - val_accuracy: 0.9856 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0312 - accuracy: 0.9910 - val_loss: 0.0515 - val_accuracy: 0.9858 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0309 - accuracy: 0.9911 - val_loss: 0.0516 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0306 - accuracy: 0.9911 - val_loss: 0.0515 - val_accuracy: 0.9858 - lr: 0.0100\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0298 - accuracy: 0.9914 - val_loss: 0.0511 - val_accuracy: 0.9859 - lr: 1.0000e-03\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0296 - accuracy: 0.9914 - val_loss: 0.0510 - val_accuracy: 0.9859 - lr: 1.0000e-03\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0295 - accuracy: 0.9914 - val_loss: 0.0510 - val_accuracy: 0.9859 - lr: 1.0000e-03\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0294 - accuracy: 0.9915 - val_loss: 0.0510 - val_accuracy: 0.9859 - lr: 1.0000e-03\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0294 - accuracy: 0.9915 - val_loss: 0.0510 - val_accuracy: 0.9859 - lr: 1.0000e-03\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.0510 - val_accuracy: 0.9859 - lr: 1.0000e-04\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 25ms/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.0510 - val_accuracy: 0.9859 - lr: 1.0000e-04\n",
            "\n",
            "Number of units: 128.\n",
            "\n",
            "Model: \"m_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_2 (GRU)                 (None, 249, 128)          69120     \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, 249, 46)          5934      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75,054\n",
            "Trainable params: 75,054\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 3s 70ms/step - loss: 2.4105 - accuracy: 0.8579 - val_loss: 0.8333 - val_accuracy: 0.9128 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.8133 - accuracy: 0.9286 - val_loss: 0.7499 - val_accuracy: 0.9397 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.6613 - accuracy: 0.9456 - val_loss: 0.5425 - val_accuracy: 0.9514 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.4563 - accuracy: 0.9555 - val_loss: 0.3665 - val_accuracy: 0.9591 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.3102 - accuracy: 0.9622 - val_loss: 0.2575 - val_accuracy: 0.9633 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.2230 - accuracy: 0.9661 - val_loss: 0.1948 - val_accuracy: 0.9668 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1730 - accuracy: 0.9690 - val_loss: 0.1587 - val_accuracy: 0.9693 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.1433 - accuracy: 0.9716 - val_loss: 0.1368 - val_accuracy: 0.9711 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1247 - accuracy: 0.9734 - val_loss: 0.1225 - val_accuracy: 0.9726 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.1119 - accuracy: 0.9751 - val_loss: 0.1125 - val_accuracy: 0.9738 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.1025 - accuracy: 0.9766 - val_loss: 0.1044 - val_accuracy: 0.9751 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0949 - accuracy: 0.9778 - val_loss: 0.0982 - val_accuracy: 0.9760 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0886 - accuracy: 0.9789 - val_loss: 0.0929 - val_accuracy: 0.9771 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0833 - accuracy: 0.9798 - val_loss: 0.0884 - val_accuracy: 0.9778 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0787 - accuracy: 0.9805 - val_loss: 0.0845 - val_accuracy: 0.9786 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0746 - accuracy: 0.9814 - val_loss: 0.0810 - val_accuracy: 0.9790 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0710 - accuracy: 0.9820 - val_loss: 0.0779 - val_accuracy: 0.9798 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 0.0677 - accuracy: 0.9828 - val_loss: 0.0752 - val_accuracy: 0.9804 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 0.0648 - accuracy: 0.9833 - val_loss: 0.0728 - val_accuracy: 0.9807 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0622 - accuracy: 0.9839 - val_loss: 0.0708 - val_accuracy: 0.9812 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0597 - accuracy: 0.9846 - val_loss: 0.0688 - val_accuracy: 0.9817 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0575 - accuracy: 0.9850 - val_loss: 0.0669 - val_accuracy: 0.9820 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0555 - accuracy: 0.9855 - val_loss: 0.0654 - val_accuracy: 0.9824 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0537 - accuracy: 0.9860 - val_loss: 0.0639 - val_accuracy: 0.9829 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0519 - accuracy: 0.9864 - val_loss: 0.0625 - val_accuracy: 0.9831 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 1s 31ms/step - loss: 0.0503 - accuracy: 0.9869 - val_loss: 0.0614 - val_accuracy: 0.9835 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0489 - accuracy: 0.9872 - val_loss: 0.0602 - val_accuracy: 0.9838 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0475 - accuracy: 0.9875 - val_loss: 0.0590 - val_accuracy: 0.9840 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.0462 - accuracy: 0.9878 - val_loss: 0.0582 - val_accuracy: 0.9842 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0449 - accuracy: 0.9881 - val_loss: 0.0573 - val_accuracy: 0.9844 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 37ms/step - loss: 0.0438 - accuracy: 0.9884 - val_loss: 0.0564 - val_accuracy: 0.9846 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0426 - accuracy: 0.9887 - val_loss: 0.0557 - val_accuracy: 0.9847 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0416 - accuracy: 0.9890 - val_loss: 0.0549 - val_accuracy: 0.9851 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0406 - accuracy: 0.9892 - val_loss: 0.0542 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0397 - accuracy: 0.9895 - val_loss: 0.0538 - val_accuracy: 0.9854 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0388 - accuracy: 0.9896 - val_loss: 0.0532 - val_accuracy: 0.9855 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0380 - accuracy: 0.9898 - val_loss: 0.0527 - val_accuracy: 0.9856 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0371 - accuracy: 0.9901 - val_loss: 0.0521 - val_accuracy: 0.9856 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0364 - accuracy: 0.9903 - val_loss: 0.0517 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.0513 - val_accuracy: 0.9858 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0349 - accuracy: 0.9906 - val_loss: 0.0508 - val_accuracy: 0.9860 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0342 - accuracy: 0.9908 - val_loss: 0.0506 - val_accuracy: 0.9861 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0336 - accuracy: 0.9910 - val_loss: 0.0501 - val_accuracy: 0.9862 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0328 - accuracy: 0.9911 - val_loss: 0.0502 - val_accuracy: 0.9861 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0322 - accuracy: 0.9913 - val_loss: 0.0497 - val_accuracy: 0.9863 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0317 - accuracy: 0.9914 - val_loss: 0.0496 - val_accuracy: 0.9864 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0311 - accuracy: 0.9916 - val_loss: 0.0493 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0305 - accuracy: 0.9917 - val_loss: 0.0492 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0300 - accuracy: 0.9918 - val_loss: 0.0489 - val_accuracy: 0.9866 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0295 - accuracy: 0.9919 - val_loss: 0.0488 - val_accuracy: 0.9866 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0290 - accuracy: 0.9921 - val_loss: 0.0485 - val_accuracy: 0.9866 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0285 - accuracy: 0.9922 - val_loss: 0.0485 - val_accuracy: 0.9866 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0280 - accuracy: 0.9924 - val_loss: 0.0483 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0276 - accuracy: 0.9924 - val_loss: 0.0483 - val_accuracy: 0.9866 - lr: 0.0100\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0272 - accuracy: 0.9926 - val_loss: 0.0482 - val_accuracy: 0.9866 - lr: 0.0100\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0267 - accuracy: 0.9927 - val_loss: 0.0481 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0262 - accuracy: 0.9928 - val_loss: 0.0483 - val_accuracy: 0.9868 - lr: 0.0100\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0258 - accuracy: 0.9930 - val_loss: 0.0480 - val_accuracy: 0.9868 - lr: 0.0100\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0254 - accuracy: 0.9930 - val_loss: 0.0481 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0245 - accuracy: 0.9933 - val_loss: 0.0474 - val_accuracy: 0.9870 - lr: 1.0000e-03\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0242 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-03\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0241 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-03\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0240 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-03\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-04\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-04\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-04\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-05\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-05\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-05\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-06\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0239 - accuracy: 0.9934 - val_loss: 0.0473 - val_accuracy: 0.9870 - lr: 1.0000e-06\n",
            "\n",
            "Number of units: 256.\n",
            "\n",
            "Model: \"m_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_3 (GRU)                 (None, 249, 256)          236544    \n",
            "                                                                 \n",
            " time_distributed_7 (TimeDis  (None, 249, 46)          11822     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 248,366\n",
            "Trainable params: 248,366\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 3s 86ms/step - loss: 3.1907 - accuracy: 0.8588 - val_loss: 2.1170 - val_accuracy: 0.9066 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 2.0364 - accuracy: 0.9172 - val_loss: 1.7533 - val_accuracy: 0.9339 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 1.4766 - accuracy: 0.9378 - val_loss: 1.1701 - val_accuracy: 0.9446 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.9899 - accuracy: 0.9475 - val_loss: 0.8040 - val_accuracy: 0.9520 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.6948 - accuracy: 0.9552 - val_loss: 0.5821 - val_accuracy: 0.9590 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.5114 - accuracy: 0.9611 - val_loss: 0.4387 - val_accuracy: 0.9626 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.3899 - accuracy: 0.9649 - val_loss: 0.3419 - val_accuracy: 0.9655 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.3073 - accuracy: 0.9675 - val_loss: 0.2755 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.2501 - accuracy: 0.9694 - val_loss: 0.2288 - val_accuracy: 0.9692 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.2095 - accuracy: 0.9710 - val_loss: 0.1960 - val_accuracy: 0.9707 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1805 - accuracy: 0.9722 - val_loss: 0.1716 - val_accuracy: 0.9719 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1589 - accuracy: 0.9735 - val_loss: 0.1538 - val_accuracy: 0.9726 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.1427 - accuracy: 0.9742 - val_loss: 0.1402 - val_accuracy: 0.9732 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1301 - accuracy: 0.9753 - val_loss: 0.1292 - val_accuracy: 0.9742 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1200 - accuracy: 0.9760 - val_loss: 0.1205 - val_accuracy: 0.9749 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1120 - accuracy: 0.9767 - val_loss: 0.1134 - val_accuracy: 0.9752 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1053 - accuracy: 0.9774 - val_loss: 0.1075 - val_accuracy: 0.9760 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0995 - accuracy: 0.9781 - val_loss: 0.1025 - val_accuracy: 0.9766 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0945 - accuracy: 0.9786 - val_loss: 0.0987 - val_accuracy: 0.9769 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0902 - accuracy: 0.9792 - val_loss: 0.0948 - val_accuracy: 0.9776 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0866 - accuracy: 0.9796 - val_loss: 0.0913 - val_accuracy: 0.9778 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0835 - accuracy: 0.9800 - val_loss: 0.0885 - val_accuracy: 0.9782 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0804 - accuracy: 0.9804 - val_loss: 0.0859 - val_accuracy: 0.9785 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0777 - accuracy: 0.9809 - val_loss: 0.0836 - val_accuracy: 0.9785 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0754 - accuracy: 0.9813 - val_loss: 0.0814 - val_accuracy: 0.9790 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0732 - accuracy: 0.9817 - val_loss: 0.0796 - val_accuracy: 0.9793 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0712 - accuracy: 0.9819 - val_loss: 0.0782 - val_accuracy: 0.9796 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0694 - accuracy: 0.9823 - val_loss: 0.0764 - val_accuracy: 0.9799 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0678 - accuracy: 0.9825 - val_loss: 0.0751 - val_accuracy: 0.9802 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0661 - accuracy: 0.9828 - val_loss: 0.0735 - val_accuracy: 0.9806 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0647 - accuracy: 0.9831 - val_loss: 0.0725 - val_accuracy: 0.9808 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 1s 49ms/step - loss: 0.0632 - accuracy: 0.9835 - val_loss: 0.0710 - val_accuracy: 0.9809 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0620 - accuracy: 0.9837 - val_loss: 0.0699 - val_accuracy: 0.9812 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 1s 50ms/step - loss: 0.0607 - accuracy: 0.9839 - val_loss: 0.0689 - val_accuracy: 0.9814 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0595 - accuracy: 0.9842 - val_loss: 0.0680 - val_accuracy: 0.9815 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0584 - accuracy: 0.9845 - val_loss: 0.0671 - val_accuracy: 0.9818 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0573 - accuracy: 0.9848 - val_loss: 0.0664 - val_accuracy: 0.9817 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0565 - accuracy: 0.9849 - val_loss: 0.0657 - val_accuracy: 0.9818 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0555 - accuracy: 0.9850 - val_loss: 0.0646 - val_accuracy: 0.9822 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0544 - accuracy: 0.9853 - val_loss: 0.0638 - val_accuracy: 0.9824 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0537 - accuracy: 0.9855 - val_loss: 0.0636 - val_accuracy: 0.9824 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0530 - accuracy: 0.9856 - val_loss: 0.0630 - val_accuracy: 0.9826 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0520 - accuracy: 0.9859 - val_loss: 0.0626 - val_accuracy: 0.9827 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0514 - accuracy: 0.9860 - val_loss: 0.0618 - val_accuracy: 0.9829 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0507 - accuracy: 0.9862 - val_loss: 0.0611 - val_accuracy: 0.9831 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0497 - accuracy: 0.9864 - val_loss: 0.0609 - val_accuracy: 0.9832 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0491 - accuracy: 0.9866 - val_loss: 0.0602 - val_accuracy: 0.9833 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0485 - accuracy: 0.9867 - val_loss: 0.0592 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0478 - accuracy: 0.9869 - val_loss: 0.0592 - val_accuracy: 0.9835 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0474 - accuracy: 0.9869 - val_loss: 0.0592 - val_accuracy: 0.9834 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0468 - accuracy: 0.9871 - val_loss: 0.0582 - val_accuracy: 0.9837 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0462 - accuracy: 0.9872 - val_loss: 0.0580 - val_accuracy: 0.9840 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0458 - accuracy: 0.9874 - val_loss: 0.0583 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0452 - accuracy: 0.9876 - val_loss: 0.0577 - val_accuracy: 0.9841 - lr: 0.0100\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0446 - accuracy: 0.9876 - val_loss: 0.0573 - val_accuracy: 0.9841 - lr: 0.0100\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0439 - accuracy: 0.9878 - val_loss: 0.0566 - val_accuracy: 0.9844 - lr: 0.0100\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0433 - accuracy: 0.9880 - val_loss: 0.0565 - val_accuracy: 0.9843 - lr: 0.0100\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0429 - accuracy: 0.9881 - val_loss: 0.0559 - val_accuracy: 0.9843 - lr: 0.0100\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0423 - accuracy: 0.9882 - val_loss: 0.0558 - val_accuracy: 0.9843 - lr: 0.0100\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0418 - accuracy: 0.9883 - val_loss: 0.0552 - val_accuracy: 0.9848 - lr: 0.0100\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0414 - accuracy: 0.9885 - val_loss: 0.0557 - val_accuracy: 0.9846 - lr: 0.0100\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0410 - accuracy: 0.9886 - val_loss: 0.0551 - val_accuracy: 0.9846 - lr: 0.0100\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0405 - accuracy: 0.9888 - val_loss: 0.0545 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0402 - accuracy: 0.9888 - val_loss: 0.0541 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0397 - accuracy: 0.9889 - val_loss: 0.0542 - val_accuracy: 0.9850 - lr: 0.0100\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0393 - accuracy: 0.9890 - val_loss: 0.0535 - val_accuracy: 0.9851 - lr: 0.0100\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0388 - accuracy: 0.9892 - val_loss: 0.0535 - val_accuracy: 0.9851 - lr: 0.0100\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0388 - accuracy: 0.9891 - val_loss: 0.0534 - val_accuracy: 0.9850 - lr: 0.0100\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0382 - accuracy: 0.9894 - val_loss: 0.0535 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0378 - accuracy: 0.9894 - val_loss: 0.0527 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 0.0527 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0369 - accuracy: 0.9896 - val_loss: 0.0528 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0371 - accuracy: 0.9896 - val_loss: 0.0528 - val_accuracy: 0.9852 - lr: 0.0100\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0354 - accuracy: 0.9900 - val_loss: 0.0511 - val_accuracy: 0.9856 - lr: 1.0000e-03\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0347 - accuracy: 0.9902 - val_loss: 0.0509 - val_accuracy: 0.9856 - lr: 1.0000e-03\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.0345 - accuracy: 0.9902 - val_loss: 0.0509 - val_accuracy: 0.9856 - lr: 1.0000e-03\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0345 - accuracy: 0.9902 - val_loss: 0.0508 - val_accuracy: 0.9856 - lr: 1.0000e-03\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0345 - accuracy: 0.9902 - val_loss: 0.0508 - val_accuracy: 0.9856 - lr: 1.0000e-03\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0344 - accuracy: 0.9903 - val_loss: 0.0507 - val_accuracy: 0.9856 - lr: 1.0000e-03\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0344 - accuracy: 0.9902 - val_loss: 0.0508 - val_accuracy: 0.9857 - lr: 1.0000e-03\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0343 - accuracy: 0.9903 - val_loss: 0.0507 - val_accuracy: 0.9856 - lr: 1.0000e-04\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0506 - val_accuracy: 0.9856 - lr: 1.0000e-04\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0507 - val_accuracy: 0.9856 - lr: 1.0000e-04\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0507 - val_accuracy: 0.9856 - lr: 1.0000e-04\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0507 - val_accuracy: 0.9856 - lr: 1.0000e-04\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0507 - val_accuracy: 0.9856 - lr: 1.0000e-05\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0507 - val_accuracy: 0.9856 - lr: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing best model.\n",
        "_, gru_f1_scores, models[models_name[1]] = get_best_model(gru_models, gru_units)"
      ],
      "metadata": {
        "id": "Ul1WbFDkOnYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a879ac-a4f3-45d0-8ffd-3e147ea42f2d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 1s 5ms/step\n",
            "41/41 [==============================] - 1s 5ms/step\n",
            "41/41 [==============================] - 1s 6ms/step\n",
            "41/41 [==============================] - 1s 7ms/step\n",
            "The best number of units is: 256.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_training_loss(models_name[1], gru_units, gru_model_histories)"
      ],
      "metadata": {
        "id": "LWsBxGmyc9PZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "39d56382-bb58-48b2-d6cf-c8907fba8114"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting scores - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_f1_scores(models_name[1], gru_units, gru_f1_scores)"
      ],
      "metadata": {
        "id": "10CECwq1lZHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "eccba348-2c59-4e23-f94b-db5f6fc3daca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Double BiLSTM Model ($m_2$)"
      ],
      "metadata": {
        "id": "JVqtTIGn9qUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible units.\n",
        "double_lstm_units = [32, 64, 128, 256]\n",
        "\n",
        "# Computing models and histories.\n",
        "double_lstm_models, double_lstm_model_histories = grid_search(models_name[2], double_lstm_units, baseline_best_units)"
      ],
      "metadata": {
        "id": "qsv40lpYOzy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d2afdf-f4c1-4cd1-93b3-fea000a0b221"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid-search, m_2 model.\n",
            "\n",
            "Number of units: 32.\n",
            "\n",
            "Model: \"m_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_4 (Bidirectio  (None, 249, 512)         628736    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 249, 64)          139520    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_8 (TimeDis  (None, 249, 46)          2990      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 771,246\n",
            "Trainable params: 771,246\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 10s 225ms/step - loss: 2.4332 - accuracy: 0.8810 - val_loss: 0.7458 - val_accuracy: 0.9170 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.5144 - accuracy: 0.9224 - val_loss: 0.3566 - val_accuracy: 0.9307 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.3004 - accuracy: 0.9335 - val_loss: 0.2498 - val_accuracy: 0.9370 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.2223 - accuracy: 0.9444 - val_loss: 0.1984 - val_accuracy: 0.9495 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1780 - accuracy: 0.9545 - val_loss: 0.1624 - val_accuracy: 0.9587 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1456 - accuracy: 0.9627 - val_loss: 0.1348 - val_accuracy: 0.9650 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.1211 - accuracy: 0.9693 - val_loss: 0.1144 - val_accuracy: 0.9705 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.1023 - accuracy: 0.9742 - val_loss: 0.0992 - val_accuracy: 0.9743 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0879 - accuracy: 0.9776 - val_loss: 0.0874 - val_accuracy: 0.9772 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0763 - accuracy: 0.9807 - val_loss: 0.0786 - val_accuracy: 0.9796 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 0.0673 - accuracy: 0.9831 - val_loss: 0.0708 - val_accuracy: 0.9820 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0596 - accuracy: 0.9850 - val_loss: 0.0653 - val_accuracy: 0.9831 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0537 - accuracy: 0.9865 - val_loss: 0.0614 - val_accuracy: 0.9839 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0484 - accuracy: 0.9879 - val_loss: 0.0572 - val_accuracy: 0.9848 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0434 - accuracy: 0.9893 - val_loss: 0.0532 - val_accuracy: 0.9860 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 0.0389 - accuracy: 0.9906 - val_loss: 0.0501 - val_accuracy: 0.9868 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0479 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0323 - accuracy: 0.9924 - val_loss: 0.0463 - val_accuracy: 0.9878 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0295 - accuracy: 0.9932 - val_loss: 0.0445 - val_accuracy: 0.9883 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0265 - accuracy: 0.9940 - val_loss: 0.0432 - val_accuracy: 0.9887 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0245 - accuracy: 0.9945 - val_loss: 0.0433 - val_accuracy: 0.9887 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0228 - accuracy: 0.9950 - val_loss: 0.0416 - val_accuracy: 0.9891 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 0.0208 - accuracy: 0.9955 - val_loss: 0.0407 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.0188 - accuracy: 0.9961 - val_loss: 0.0399 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0178 - accuracy: 0.9963 - val_loss: 0.0406 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 2s 130ms/step - loss: 0.0165 - accuracy: 0.9966 - val_loss: 0.0386 - val_accuracy: 0.9899 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0147 - accuracy: 0.9971 - val_loss: 0.0394 - val_accuracy: 0.9897 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0136 - accuracy: 0.9974 - val_loss: 0.0382 - val_accuracy: 0.9900 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0123 - accuracy: 0.9978 - val_loss: 0.0383 - val_accuracy: 0.9900 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0117 - accuracy: 0.9979 - val_loss: 0.0379 - val_accuracy: 0.9903 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.0382 - val_accuracy: 0.9901 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.0100 - accuracy: 0.9982 - val_loss: 0.0379 - val_accuracy: 0.9902 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.0090 - accuracy: 0.9984 - val_loss: 0.0381 - val_accuracy: 0.9903 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.0370 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.0370 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.0070 - accuracy: 0.9988 - val_loss: 0.0369 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.0370 - val_accuracy: 0.9903 - lr: 1.0000e-03\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.0370 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.0370 - val_accuracy: 0.9903 - lr: 1.0000e-03\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 0.0370 - val_accuracy: 0.9903 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 0.0370 - val_accuracy: 0.9904 - lr: 1.0000e-04\n",
            "\n",
            "Number of units: 64.\n",
            "\n",
            "Model: \"m_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_6 (Bidirectio  (None, 249, 512)         628736    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 249, 128)         295424    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, 249, 46)          5934      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 930,094\n",
            "Trainable params: 930,094\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 9s 234ms/step - loss: 2.5758 - accuracy: 0.8887 - val_loss: 1.0177 - val_accuracy: 0.9285 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 2s 139ms/step - loss: 0.6590 - accuracy: 0.9340 - val_loss: 0.3895 - val_accuracy: 0.9451 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 2s 140ms/step - loss: 0.2845 - accuracy: 0.9507 - val_loss: 0.2007 - val_accuracy: 0.9570 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.1632 - accuracy: 0.9632 - val_loss: 0.1356 - val_accuracy: 0.9672 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.1150 - accuracy: 0.9707 - val_loss: 0.1031 - val_accuracy: 0.9727 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0896 - accuracy: 0.9758 - val_loss: 0.0844 - val_accuracy: 0.9769 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 2s 142ms/step - loss: 0.0737 - accuracy: 0.9796 - val_loss: 0.0734 - val_accuracy: 0.9799 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0624 - accuracy: 0.9831 - val_loss: 0.0639 - val_accuracy: 0.9826 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0539 - accuracy: 0.9853 - val_loss: 0.0568 - val_accuracy: 0.9843 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0473 - accuracy: 0.9872 - val_loss: 0.0548 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0428 - accuracy: 0.9884 - val_loss: 0.0494 - val_accuracy: 0.9863 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0379 - accuracy: 0.9898 - val_loss: 0.0457 - val_accuracy: 0.9872 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0340 - accuracy: 0.9909 - val_loss: 0.0443 - val_accuracy: 0.9875 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 2s 146ms/step - loss: 0.0304 - accuracy: 0.9920 - val_loss: 0.0422 - val_accuracy: 0.9880 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.0273 - accuracy: 0.9928 - val_loss: 0.0400 - val_accuracy: 0.9886 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 2s 146ms/step - loss: 0.0247 - accuracy: 0.9936 - val_loss: 0.0391 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 2s 146ms/step - loss: 0.0231 - accuracy: 0.9941 - val_loss: 0.0374 - val_accuracy: 0.9894 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 2s 147ms/step - loss: 0.0205 - accuracy: 0.9949 - val_loss: 0.0367 - val_accuracy: 0.9898 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 2s 146ms/step - loss: 0.0183 - accuracy: 0.9956 - val_loss: 0.0361 - val_accuracy: 0.9900 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0166 - accuracy: 0.9960 - val_loss: 0.0363 - val_accuracy: 0.9901 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 2s 145ms/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 0.0363 - val_accuracy: 0.9900 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0141 - accuracy: 0.9968 - val_loss: 0.0362 - val_accuracy: 0.9902 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0119 - accuracy: 0.9974 - val_loss: 0.0339 - val_accuracy: 0.9905 - lr: 1.0000e-03\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0110 - accuracy: 0.9976 - val_loss: 0.0337 - val_accuracy: 0.9906 - lr: 1.0000e-03\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.0338 - val_accuracy: 0.9906 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0106 - accuracy: 0.9977 - val_loss: 0.0336 - val_accuracy: 0.9906 - lr: 1.0000e-03\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 2s 141ms/step - loss: 0.0105 - accuracy: 0.9977 - val_loss: 0.0337 - val_accuracy: 0.9907 - lr: 1.0000e-03\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 2s 144ms/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.0337 - val_accuracy: 0.9906 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 2s 142ms/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.0336 - val_accuracy: 0.9906 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 2s 142ms/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.0337 - val_accuracy: 0.9906 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 2s 143ms/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.0337 - val_accuracy: 0.9906 - lr: 1.0000e-05\n",
            "\n",
            "Number of units: 128.\n",
            "\n",
            "Model: \"m_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_8 (Bidirectio  (None, 249, 512)         628736    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 249, 256)         656384    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_10 (TimeDi  (None, 249, 46)          11822     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,296,942\n",
            "Trainable params: 1,296,942\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 10s 266ms/step - loss: 3.1768 - accuracy: 0.8802 - val_loss: 1.4553 - val_accuracy: 0.9293 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 3s 174ms/step - loss: 0.9046 - accuracy: 0.9349 - val_loss: 0.4712 - val_accuracy: 0.9468 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 3s 174ms/step - loss: 0.3232 - accuracy: 0.9524 - val_loss: 0.2067 - val_accuracy: 0.9572 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.1579 - accuracy: 0.9648 - val_loss: 0.1231 - val_accuracy: 0.9689 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.1028 - accuracy: 0.9734 - val_loss: 0.0924 - val_accuracy: 0.9750 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.0796 - accuracy: 0.9782 - val_loss: 0.0763 - val_accuracy: 0.9782 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0650 - accuracy: 0.9816 - val_loss: 0.0657 - val_accuracy: 0.9815 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0541 - accuracy: 0.9850 - val_loss: 0.0572 - val_accuracy: 0.9840 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0464 - accuracy: 0.9872 - val_loss: 0.0525 - val_accuracy: 0.9850 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0409 - accuracy: 0.9886 - val_loss: 0.0476 - val_accuracy: 0.9864 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0362 - accuracy: 0.9898 - val_loss: 0.0443 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0321 - accuracy: 0.9911 - val_loss: 0.0418 - val_accuracy: 0.9881 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0290 - accuracy: 0.9921 - val_loss: 0.0400 - val_accuracy: 0.9886 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 3s 182ms/step - loss: 0.0260 - accuracy: 0.9929 - val_loss: 0.0386 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 3s 179ms/step - loss: 0.0230 - accuracy: 0.9940 - val_loss: 0.0371 - val_accuracy: 0.9892 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.0207 - accuracy: 0.9947 - val_loss: 0.0356 - val_accuracy: 0.9899 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.0345 - val_accuracy: 0.9902 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0165 - accuracy: 0.9959 - val_loss: 0.0349 - val_accuracy: 0.9902 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0151 - accuracy: 0.9962 - val_loss: 0.0351 - val_accuracy: 0.9902 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0135 - accuracy: 0.9967 - val_loss: 0.0354 - val_accuracy: 0.9903 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0111 - accuracy: 0.9974 - val_loss: 0.0330 - val_accuracy: 0.9907 - lr: 1.0000e-03\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0102 - accuracy: 0.9977 - val_loss: 0.0327 - val_accuracy: 0.9908 - lr: 1.0000e-03\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0100 - accuracy: 0.9977 - val_loss: 0.0328 - val_accuracy: 0.9908 - lr: 1.0000e-03\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0098 - accuracy: 0.9978 - val_loss: 0.0327 - val_accuracy: 0.9908 - lr: 1.0000e-03\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0096 - accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.9907 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.9908 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.0094 - accuracy: 0.9980 - val_loss: 0.0327 - val_accuracy: 0.9908 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 3s 176ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.0327 - val_accuracy: 0.9908 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.0327 - val_accuracy: 0.9908 - lr: 1.0000e-05\n",
            "\n",
            "Number of units: 256.\n",
            "\n",
            "Model: \"m_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_10 (Bidirecti  (None, 249, 512)         628736    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 249, 512)         1574912   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_11 (TimeDi  (None, 249, 46)          23598     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,227,246\n",
            "Trainable params: 2,227,246\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 11s 326ms/step - loss: 4.9793 - accuracy: 0.7717 - val_loss: 2.9831 - val_accuracy: 0.9161 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 4s 235ms/step - loss: 1.7596 - accuracy: 0.9190 - val_loss: 0.7971 - val_accuracy: 0.9231 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 4s 234ms/step - loss: 0.5412 - accuracy: 0.9272 - val_loss: 0.3529 - val_accuracy: 0.9306 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 4s 236ms/step - loss: 0.2901 - accuracy: 0.9322 - val_loss: 0.2401 - val_accuracy: 0.9391 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 4s 236ms/step - loss: 0.2143 - accuracy: 0.9433 - val_loss: 0.1922 - val_accuracy: 0.9503 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.1739 - accuracy: 0.9536 - val_loss: 0.1604 - val_accuracy: 0.9566 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.1449 - accuracy: 0.9612 - val_loss: 0.1358 - val_accuracy: 0.9629 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 4s 242ms/step - loss: 0.1221 - accuracy: 0.9673 - val_loss: 0.1174 - val_accuracy: 0.9669 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 4s 242ms/step - loss: 0.1054 - accuracy: 0.9712 - val_loss: 0.1040 - val_accuracy: 0.9715 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 4s 243ms/step - loss: 0.0924 - accuracy: 0.9747 - val_loss: 0.0932 - val_accuracy: 0.9736 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 4s 243ms/step - loss: 0.0824 - accuracy: 0.9773 - val_loss: 0.0839 - val_accuracy: 0.9766 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 4s 244ms/step - loss: 0.0738 - accuracy: 0.9796 - val_loss: 0.0762 - val_accuracy: 0.9782 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 4s 242ms/step - loss: 0.0669 - accuracy: 0.9812 - val_loss: 0.0711 - val_accuracy: 0.9796 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 4s 243ms/step - loss: 0.0607 - accuracy: 0.9832 - val_loss: 0.0664 - val_accuracy: 0.9812 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 4s 240ms/step - loss: 0.0559 - accuracy: 0.9846 - val_loss: 0.0616 - val_accuracy: 0.9826 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0520 - accuracy: 0.9856 - val_loss: 0.0584 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0487 - accuracy: 0.9866 - val_loss: 0.0555 - val_accuracy: 0.9845 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0450 - accuracy: 0.9878 - val_loss: 0.0547 - val_accuracy: 0.9847 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 4s 236ms/step - loss: 0.0419 - accuracy: 0.9886 - val_loss: 0.0505 - val_accuracy: 0.9860 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0390 - accuracy: 0.9895 - val_loss: 0.0493 - val_accuracy: 0.9862 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 4s 235ms/step - loss: 0.0373 - accuracy: 0.9899 - val_loss: 0.0507 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0356 - accuracy: 0.9903 - val_loss: 0.0472 - val_accuracy: 0.9867 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 4s 236ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 0.0444 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0303 - accuracy: 0.9919 - val_loss: 0.0430 - val_accuracy: 0.9880 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0290 - accuracy: 0.9923 - val_loss: 0.0423 - val_accuracy: 0.9881 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0269 - accuracy: 0.9930 - val_loss: 0.0418 - val_accuracy: 0.9883 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0255 - accuracy: 0.9933 - val_loss: 0.0411 - val_accuracy: 0.9885 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 4s 275ms/step - loss: 0.0240 - accuracy: 0.9938 - val_loss: 0.0396 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0227 - accuracy: 0.9941 - val_loss: 0.0398 - val_accuracy: 0.9889 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0221 - accuracy: 0.9943 - val_loss: 0.0423 - val_accuracy: 0.9884 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0214 - accuracy: 0.9944 - val_loss: 0.0393 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0200 - accuracy: 0.9950 - val_loss: 0.0394 - val_accuracy: 0.9892 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.0385 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0178 - accuracy: 0.9956 - val_loss: 0.0393 - val_accuracy: 0.9892 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0164 - accuracy: 0.9960 - val_loss: 0.0376 - val_accuracy: 0.9899 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0153 - accuracy: 0.9963 - val_loss: 0.0382 - val_accuracy: 0.9896 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0144 - accuracy: 0.9966 - val_loss: 0.0368 - val_accuracy: 0.9901 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0132 - accuracy: 0.9971 - val_loss: 0.0375 - val_accuracy: 0.9898 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0125 - accuracy: 0.9972 - val_loss: 0.0368 - val_accuracy: 0.9901 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0117 - accuracy: 0.9974 - val_loss: 0.0370 - val_accuracy: 0.9901 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0101 - accuracy: 0.9979 - val_loss: 0.0356 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 4s 240ms/step - loss: 0.0094 - accuracy: 0.9980 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0092 - accuracy: 0.9981 - val_loss: 0.0356 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0091 - accuracy: 0.9981 - val_loss: 0.0356 - val_accuracy: 0.9904 - lr: 1.0000e-03\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0090 - accuracy: 0.9982 - val_loss: 0.0356 - val_accuracy: 0.9904 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 4s 239ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 4s 237ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 4s 236ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-06\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 4s 238ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0355 - val_accuracy: 0.9904 - lr: 1.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing best model.\n",
        "_, double_lstm_f1_scores, models[models_name[2]] = get_best_model(double_lstm_models, double_lstm_units)"
      ],
      "metadata": {
        "id": "_-HZGh7gPCZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c2e69a-150f-463d-9a0e-fa232a3cf7bd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 2s 21ms/step\n",
            "41/41 [==============================] - 2s 22ms/step\n",
            "41/41 [==============================] - 3s 25ms/step\n",
            "41/41 [==============================] - 2s 25ms/step\n",
            "The best number of units is: 128.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_training_loss(models_name[2], double_lstm_units, double_lstm_model_histories)"
      ],
      "metadata": {
        "id": "nXa5naV_dDPV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6eac1f90-c147-44ff-c698-36f89dc373ec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting scores - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_f1_scores(models_name[2], double_lstm_units, double_lstm_f1_scores)"
      ],
      "metadata": {
        "id": "zm-3dM5PldPF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "cfb2681e-39ef-40de-ce9b-eab7af4c89b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Double Dense Layer Model ($m_3$)"
      ],
      "metadata": {
        "id": "GIOxs5nNC3yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible units.\n",
        "double_dense_units = [32, 64, 128, 256]\n",
        "\n",
        "# Computing models and histories.\n",
        "double_dense_models, double_dense_model_histories = grid_search(models_name[3], double_dense_units, baseline_best_units)"
      ],
      "metadata": {
        "id": "8wPBLVr5PeUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a12ab7-bff1-4eb3-a0d6-56bdb93cf918"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid-search, m_3 model.\n",
            "\n",
            "Number of units: 32.\n",
            "\n",
            "Model: \"m_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_12 (Bidirecti  (None, 249, 512)         628736    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_12 (TimeDi  (None, 249, 32)          16416     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 249, 32)           0         \n",
            "                                                                 \n",
            " time_distributed_13 (TimeDi  (None, 249, 46)          1518      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 646,670\n",
            "Trainable params: 646,670\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 6s 154ms/step - loss: 2.3945 - accuracy: 0.8873 - val_loss: 0.7875 - val_accuracy: 0.9326 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 93ms/step - loss: 0.4876 - accuracy: 0.9422 - val_loss: 0.2866 - val_accuracy: 0.9530 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 92ms/step - loss: 0.2105 - accuracy: 0.9587 - val_loss: 0.1540 - val_accuracy: 0.9636 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 92ms/step - loss: 0.1245 - accuracy: 0.9688 - val_loss: 0.1086 - val_accuracy: 0.9713 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 93ms/step - loss: 0.0916 - accuracy: 0.9749 - val_loss: 0.0865 - val_accuracy: 0.9755 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0740 - accuracy: 0.9795 - val_loss: 0.0735 - val_accuracy: 0.9791 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0624 - accuracy: 0.9824 - val_loss: 0.0652 - val_accuracy: 0.9814 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 93ms/step - loss: 0.0539 - accuracy: 0.9849 - val_loss: 0.0580 - val_accuracy: 0.9832 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0471 - accuracy: 0.9867 - val_loss: 0.0540 - val_accuracy: 0.9844 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0419 - accuracy: 0.9883 - val_loss: 0.0511 - val_accuracy: 0.9854 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0383 - accuracy: 0.9892 - val_loss: 0.0490 - val_accuracy: 0.9859 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0349 - accuracy: 0.9901 - val_loss: 0.0456 - val_accuracy: 0.9868 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0316 - accuracy: 0.9910 - val_loss: 0.0441 - val_accuracy: 0.9874 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 96ms/step - loss: 0.0284 - accuracy: 0.9921 - val_loss: 0.0433 - val_accuracy: 0.9877 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0260 - accuracy: 0.9928 - val_loss: 0.0414 - val_accuracy: 0.9881 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0236 - accuracy: 0.9935 - val_loss: 0.0412 - val_accuracy: 0.9885 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0219 - accuracy: 0.9941 - val_loss: 0.0393 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0199 - accuracy: 0.9946 - val_loss: 0.0384 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0181 - accuracy: 0.9952 - val_loss: 0.0389 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.0396 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.0379 - val_accuracy: 0.9896 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.0141 - accuracy: 0.9964 - val_loss: 0.0380 - val_accuracy: 0.9897 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0399 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.0398 - val_accuracy: 0.9894 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0100 - accuracy: 0.9976 - val_loss: 0.0372 - val_accuracy: 0.9900 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0090 - accuracy: 0.9979 - val_loss: 0.0368 - val_accuracy: 0.9900 - lr: 1.0000e-03\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.0369 - val_accuracy: 0.9900 - lr: 1.0000e-03\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.0367 - val_accuracy: 0.9900 - lr: 1.0000e-03\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0085 - accuracy: 0.9980 - val_loss: 0.0369 - val_accuracy: 0.9901 - lr: 1.0000e-03\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0083 - accuracy: 0.9981 - val_loss: 0.0369 - val_accuracy: 0.9900 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 0.0370 - val_accuracy: 0.9900 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 0.0369 - val_accuracy: 0.9900 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 0.0369 - val_accuracy: 0.9901 - lr: 1.0000e-05\n",
            "\n",
            "Number of units: 64.\n",
            "\n",
            "Model: \"m_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_13 (Bidirecti  (None, 249, 512)         628736    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_14 (TimeDi  (None, 249, 64)          32832     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 249, 64)           0         \n",
            "                                                                 \n",
            " time_distributed_15 (TimeDi  (None, 249, 46)          2990      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 664,558\n",
            "Trainable params: 664,558\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 6s 156ms/step - loss: 2.4007 - accuracy: 0.8975 - val_loss: 0.9780 - val_accuracy: 0.9349 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.5956 - accuracy: 0.9428 - val_loss: 0.3318 - val_accuracy: 0.9533 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.2308 - accuracy: 0.9600 - val_loss: 0.1535 - val_accuracy: 0.9650 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 93ms/step - loss: 0.1210 - accuracy: 0.9709 - val_loss: 0.1029 - val_accuracy: 0.9735 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 93ms/step - loss: 0.0858 - accuracy: 0.9766 - val_loss: 0.0800 - val_accuracy: 0.9779 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0688 - accuracy: 0.9805 - val_loss: 0.0701 - val_accuracy: 0.9800 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0588 - accuracy: 0.9832 - val_loss: 0.0633 - val_accuracy: 0.9820 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0517 - accuracy: 0.9853 - val_loss: 0.0572 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0458 - accuracy: 0.9869 - val_loss: 0.0556 - val_accuracy: 0.9836 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0419 - accuracy: 0.9879 - val_loss: 0.0502 - val_accuracy: 0.9853 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 94ms/step - loss: 0.0383 - accuracy: 0.9891 - val_loss: 0.0479 - val_accuracy: 0.9861 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0462 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0323 - accuracy: 0.9906 - val_loss: 0.0439 - val_accuracy: 0.9872 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.0422 - val_accuracy: 0.9877 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0267 - accuracy: 0.9924 - val_loss: 0.0411 - val_accuracy: 0.9879 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0246 - accuracy: 0.9931 - val_loss: 0.0409 - val_accuracy: 0.9882 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.0229 - accuracy: 0.9937 - val_loss: 0.0402 - val_accuracy: 0.9883 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0210 - accuracy: 0.9942 - val_loss: 0.0402 - val_accuracy: 0.9886 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0195 - accuracy: 0.9947 - val_loss: 0.0400 - val_accuracy: 0.9888 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0177 - accuracy: 0.9953 - val_loss: 0.0396 - val_accuracy: 0.9892 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0163 - accuracy: 0.9958 - val_loss: 0.0392 - val_accuracy: 0.9892 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0153 - accuracy: 0.9959 - val_loss: 0.0394 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0141 - accuracy: 0.9964 - val_loss: 0.0406 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 1s 95ms/step - loss: 0.0133 - accuracy: 0.9966 - val_loss: 0.0395 - val_accuracy: 0.9896 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0106 - accuracy: 0.9975 - val_loss: 0.0378 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.0376 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0095 - accuracy: 0.9978 - val_loss: 0.0378 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 2s 95ms/step - loss: 0.0094 - accuracy: 0.9978 - val_loss: 0.0376 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 0.0377 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.0091 - accuracy: 0.9979 - val_loss: 0.0377 - val_accuracy: 0.9899 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.0090 - accuracy: 0.9979 - val_loss: 0.0378 - val_accuracy: 0.9899 - lr: 1.0000e-04\n",
            "\n",
            "Number of units: 128.\n",
            "\n",
            "Model: \"m_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_14 (Bidirecti  (None, 249, 512)         628736    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_16 (TimeDi  (None, 249, 128)         65664     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 249, 128)          0         \n",
            "                                                                 \n",
            " time_distributed_17 (TimeDi  (None, 249, 46)          5934      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 700,334\n",
            "Trainable params: 700,334\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 5s 159ms/step - loss: 2.7525 - accuracy: 0.8832 - val_loss: 1.3881 - val_accuracy: 0.9202 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 2s 96ms/step - loss: 0.7957 - accuracy: 0.9340 - val_loss: 0.3880 - val_accuracy: 0.9475 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.2787 - accuracy: 0.9536 - val_loss: 0.1864 - val_accuracy: 0.9611 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.1452 - accuracy: 0.9663 - val_loss: 0.1200 - val_accuracy: 0.9688 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.1013 - accuracy: 0.9722 - val_loss: 0.0949 - val_accuracy: 0.9735 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 2s 97ms/step - loss: 0.0820 - accuracy: 0.9766 - val_loss: 0.0807 - val_accuracy: 0.9770 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.0699 - accuracy: 0.9801 - val_loss: 0.0722 - val_accuracy: 0.9792 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0607 - accuracy: 0.9827 - val_loss: 0.0651 - val_accuracy: 0.9813 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0538 - accuracy: 0.9845 - val_loss: 0.0607 - val_accuracy: 0.9828 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0487 - accuracy: 0.9860 - val_loss: 0.0557 - val_accuracy: 0.9839 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0450 - accuracy: 0.9869 - val_loss: 0.0524 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 2s 105ms/step - loss: 0.0409 - accuracy: 0.9882 - val_loss: 0.0500 - val_accuracy: 0.9858 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0380 - accuracy: 0.9890 - val_loss: 0.0488 - val_accuracy: 0.9859 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0355 - accuracy: 0.9898 - val_loss: 0.0468 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0323 - accuracy: 0.9906 - val_loss: 0.0449 - val_accuracy: 0.9870 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0298 - accuracy: 0.9915 - val_loss: 0.0427 - val_accuracy: 0.9878 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0275 - accuracy: 0.9923 - val_loss: 0.0419 - val_accuracy: 0.9880 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0256 - accuracy: 0.9928 - val_loss: 0.0421 - val_accuracy: 0.9882 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 0.0410 - val_accuracy: 0.9886 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0223 - accuracy: 0.9938 - val_loss: 0.0415 - val_accuracy: 0.9886 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0208 - accuracy: 0.9942 - val_loss: 0.0393 - val_accuracy: 0.9891 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0195 - accuracy: 0.9946 - val_loss: 0.0401 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0181 - accuracy: 0.9950 - val_loss: 0.0404 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0166 - accuracy: 0.9956 - val_loss: 0.0398 - val_accuracy: 0.9893 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0144 - accuracy: 0.9962 - val_loss: 0.0378 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 2s 100ms/step - loss: 0.0136 - accuracy: 0.9964 - val_loss: 0.0377 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 2s 99ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 0.0377 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 2s 99ms/step - loss: 0.0132 - accuracy: 0.9965 - val_loss: 0.0376 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.0130 - accuracy: 0.9965 - val_loss: 0.0377 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.0129 - accuracy: 0.9966 - val_loss: 0.0378 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.0376 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.0376 - val_accuracy: 0.9898 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 2s 98ms/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.0377 - val_accuracy: 0.9898 - lr: 1.0000e-04\n",
            "\n",
            "Number of units: 256.\n",
            "\n",
            "Model: \"m_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_15 (Bidirecti  (None, 249, 512)         628736    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_18 (TimeDi  (None, 249, 256)         131328    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 249, 256)          0         \n",
            "                                                                 \n",
            " time_distributed_19 (TimeDi  (None, 249, 46)          11822     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 249, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 771,886\n",
            "Trainable params: 771,886\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 5s 162ms/step - loss: 3.2067 - accuracy: 0.8350 - val_loss: 1.9098 - val_accuracy: 0.9275 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 1.3879 - accuracy: 0.9425 - val_loss: 0.7704 - val_accuracy: 0.9544 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.4960 - accuracy: 0.9601 - val_loss: 0.2833 - val_accuracy: 0.9650 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.2038 - accuracy: 0.9698 - val_loss: 0.1476 - val_accuracy: 0.9721 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.1175 - accuracy: 0.9759 - val_loss: 0.1018 - val_accuracy: 0.9762 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0840 - accuracy: 0.9797 - val_loss: 0.0793 - val_accuracy: 0.9794 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0666 - accuracy: 0.9825 - val_loss: 0.0675 - val_accuracy: 0.9819 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0563 - accuracy: 0.9846 - val_loss: 0.0613 - val_accuracy: 0.9825 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0486 - accuracy: 0.9865 - val_loss: 0.0560 - val_accuracy: 0.9839 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 2s 103ms/step - loss: 0.0435 - accuracy: 0.9876 - val_loss: 0.0518 - val_accuracy: 0.9849 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 2s 105ms/step - loss: 0.0391 - accuracy: 0.9888 - val_loss: 0.0487 - val_accuracy: 0.9857 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0354 - accuracy: 0.9898 - val_loss: 0.0462 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0330 - accuracy: 0.9904 - val_loss: 0.0459 - val_accuracy: 0.9865 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0304 - accuracy: 0.9913 - val_loss: 0.0447 - val_accuracy: 0.9869 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0277 - accuracy: 0.9920 - val_loss: 0.0420 - val_accuracy: 0.9878 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0252 - accuracy: 0.9928 - val_loss: 0.0405 - val_accuracy: 0.9884 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0235 - accuracy: 0.9932 - val_loss: 0.0404 - val_accuracy: 0.9882 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 0.0398 - val_accuracy: 0.9885 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0202 - accuracy: 0.9942 - val_loss: 0.0396 - val_accuracy: 0.9888 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.0388 - val_accuracy: 0.9890 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9892 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0150 - accuracy: 0.9958 - val_loss: 0.0393 - val_accuracy: 0.9895 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.0374 - val_accuracy: 0.9897 - lr: 1.0000e-03\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0370 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0369 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.0370 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.0369 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0370 - val_accuracy: 0.9899 - lr: 1.0000e-03\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0110 - accuracy: 0.9971 - val_loss: 0.0372 - val_accuracy: 0.9898 - lr: 1.0000e-03\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.0371 - val_accuracy: 0.9900 - lr: 1.0000e-03\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 2s 101ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.0370 - val_accuracy: 0.9899 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 2s 102ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.0371 - val_accuracy: 0.9899 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing best model.\n",
        "_, double_dense_f1_scores, models[models_name[3]] = get_best_model(double_dense_models, double_dense_units)"
      ],
      "metadata": {
        "id": "bSvZYbeoQTnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47bb7d04-2323-4105-f9d7-ba0177b567b4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 2s 15ms/step\n",
            "41/41 [==============================] - 1s 12ms/step\n",
            "41/41 [==============================] - 1s 12ms/step\n",
            "41/41 [==============================] - 1s 12ms/step\n",
            "The best number of units is: 256.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_training_loss(models_name[3], double_dense_units, double_dense_model_histories)"
      ],
      "metadata": {
        "id": "2a2RUiMSdRca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "9b5b5cd4-0f84-4d5f-b589-75258f8ee40e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting scores - using LaTeX :^) - and saving figure as .pdf file.\n",
        "plot_f1_scores(models_name[3], double_dense_units, double_dense_f1_scores)"
      ],
      "metadata": {
        "id": "WW_w2k22livM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "c96d7e6a-174e-49cb-8614-a11472093d2d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Models macro F1-scores.\n",
        "models_score = {}\n",
        "\n",
        "# Computing macro F1-scores on the four models selected with grid-search.\n",
        "for model in models:\n",
        "\n",
        "  # Computing macro F1-score.\n",
        "  models_score[model] = compute_F1_score(models[model], validation_features, validation_tags, tag_to_index)\n",
        "\n",
        "  # Computing and printing macro F1-score.\n",
        "  print(f\"The macro F1-score for model {model} is: {models_score[model]}.\")\n",
        "\n",
        "# Storing the two best models.\n",
        "best_models = sorted(models_score, key = models_score.get, reverse = True)[:2]"
      ],
      "metadata": {
        "id": "fcGpnTPPXLS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99dccbd4-1a83-4820-c49c-4d02b896199e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 1s 15ms/step\n",
            "The macro F1-score for model m_0 is: 0.7273296529644282.\n",
            "41/41 [==============================] - 0s 7ms/step\n",
            "The macro F1-score for model m_1 is: 0.6286681630831981.\n",
            "41/41 [==============================] - 1s 22ms/step\n",
            "The macro F1-score for model m_2 is: 0.7068958913570299.\n",
            "41/41 [==============================] - 1s 12ms/step\n",
            "The macro F1-score for model m_3 is: 0.704919738701608.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Testing"
      ],
      "metadata": {
        "id": "BXJiM4jgQuAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the two best models.\n",
        "for model in best_models:\n",
        "\n",
        "  # Computing and printing macro F1-score.\n",
        "  print(f\"The macro F1-score, on the test set, for model {model} is: {compute_F1_score(models[model], test_features, test_tags, tag_to_index)}.\")"
      ],
      "metadata": {
        "id": "Y2BQysorQxR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bf4662-6fe2-401f-c205-1f49c99aa062"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 12ms/step\n",
            "The macro F1-score, on the test set, for model m_0 is: 0.7219506363115593.\n",
            "21/21 [==============================] - 0s 22ms/step\n",
            "The macro F1-score, on the test set, for model m_2 is: 0.7248297840620403.\n"
          ]
        }
      ]
    }
  ]
}